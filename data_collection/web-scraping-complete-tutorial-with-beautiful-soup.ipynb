{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-19T06:03:13.699449Z","iopub.execute_input":"2024-11-19T06:03:13.699851Z","iopub.status.idle":"2024-11-19T06:03:15.082820Z","shell.execute_reply.started":"2024-11-19T06:03:13.699812Z","shell.execute_reply":"2024-11-19T06:03:15.081554Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Web Scraping with BeautifulSoup (Basic to Advanced)**\n\n#### **Course Outline**\n\n##### **1. Introduction to Web Scraping**\n   - **What is Web Scraping?**\n   - **Why Use Web Scraping?**\n   - **Legal Considerations and Best Practices**\n   - **Setting Up the Environment**\n     - Installing Python\n     - Installing BeautifulSoup\n     - Installing Required Libraries (Requests, lxml, etc.)\n\n##### **2. Getting Started with BeautifulSoup**\n   - **Introduction to BeautifulSoup**\n   - **Basic Syntax and Setup**\n     - Creating a Soup Object\n     - Using HTML/XML Parsers (`html.parser`, `lxml`)\n   - **Parsing a Simple HTML Document**\n   - **Navigating the Parse Tree**\n     - Accessing Tags, Attributes, and Text\n     - Using `.tagName`, `.attrs`, `.text`\n\n##### **3. Searching and Navigating with BeautifulSoup**\n   - **Finding Elements by Tag Name**\n     - `find()`\n     - `find_all()`\n   - **CSS Selectors**\n     - Using `select()`\n   - **Using Filters and Regular Expressions**\n   - **Searching with Attributes**\n     - `class_`, `id`, `href`, etc.\n   - **Navigating the Tree**\n     - `parent`, `children`, `next_sibling`, `previous_sibling`\n   - **Getting All Links and Images from a Page**\n\n##### **4. Extracting Data with BeautifulSoup**\n   - **Extracting Text from Tags**\n   - **Using `get()` Method for Attributes**\n   - **Extracting Tables**\n     - Finding and Extracting Table Data\n     - Parsing HTML Tables\n   - **Extracting Data from Nested Tags**\n   - **Handling Missing or Incomplete Data**\n\n##### **5. Advanced HTML Parsing Techniques**\n   - **Working with Complex HTML Structures**\n   - **Handling Pagination in Web Scraping**\n   - **Dealing with JavaScript-Rendered Content**\n     - Using `Selenium` for Dynamic Content\n     - Combining BeautifulSoup with Selenium\n   - **Handling AJAX Requests**\n     - Using Network Tab to Identify Requests\n\n##### **6. Data Cleaning and Manipulation**\n   - **Cleaning Extracted Data**\n   - **Removing HTML Tags and Special Characters**\n   - **Using Python's `re` Library for Text Processing**\n   - **Handling Encoding Issues**\n   - **Using Pandas to Organize Scraped Data**\n\n##### **7. Exporting and Storing Scraped Data**\n   - **Saving Data to CSV Files**\n   - **Saving Data to Excel Files**\n   - **Saving Data to JSON Files**\n   - **Storing Data in Databases (SQLite, PostgreSQL)**\n   - **Creating a Simple API for Scraped Data with Flask**\n\n##### **8. Handling Errors and Exceptions in Web Scraping**\n   - **Common Errors in BeautifulSoup**\n   - **Handling HTTP Errors with `requests` Library**\n   - **Using `try-except` Blocks for Error Handling**\n   - **Setting Up Retry Logic for Failed Requests**\n\n##### **9. Web Scraping Projects**\n   - **Project 1: Scraping E-commerce Product Data**\n   - **Project 2: Scraping News Headlines and Summaries**\n   - **Project 3: Scraping Weather Data from a Weather Website**\n   - **Project 4: Scraping Wikipedia Tables and Articles**\n\n##### **10. Advanced BeautifulSoup Techniques**\n   - **Using Proxies to Avoid IP Blocking**\n   - **Rotating User Agents with `fake_useragent` Library**\n   - **Building a Web Scraping Pipeline with Python**\n   - **Scraping Data from Websites with Infinite Scroll**\n   - **Using `concurrent.futures` for Parallel Scraping**\n\n##### **11. Web Scraping Best Practices and Optimization**\n   - **Respecting `robots.txt` and Website Terms of Service**\n   - **Using Request Headers to Avoid Blocking**\n   - **Optimizing the Scraping Process for Speed**\n   - **Data Throttling and Sleep Intervals**\n   - **Avoiding Captchas with Anti-Captcha Services**\n\n##### **12. Deploying a Web Scraping Script**\n   - **Using Cron Jobs for Scheduling**\n   - **Deploying on Cloud Services (AWS Lambda, Heroku)**\n   - **Creating a Web Scraping API with FastAPI**\n   - **Sending Notifications (Email/Slack) After Scraping**\n\n##### **13. Web Scraping Alternatives**\n   - **Comparison with Scrapy and Selenium**\n   - **When to Use BeautifulSoup, Scrapy, or Selenium**\n   - **Exploring Other Libraries (Puppeteer, Playwright)**\n\n##### **14. Course Project: Building a Complete Web Scraper**\n   - **Project Setup and Requirements**\n   - **Building the Scraper with BeautifulSoup**\n   - **Data Cleaning and Exporting**\n   - **Deploying and Automating the Web Scraper**\n   - **Project Showcase and Review**\n\n##### **15. Conclusion and Next Steps**\n   - **Recap of Key Learnings**\n   - **Resources for Further Learning**\n   - **Tips for Building Your Own Web Scrapers**\n   - **Q&A and Troubleshooting Common Issues**\n","metadata":{}},{"cell_type":"markdown","source":"## **1. Introduction to Web Scraping**\r\n\r\n### **1.1 What is Web Scraping?**\r\n\r\n**Web Scraping** is the automated process of extracting data from websites. Instead of manually copying and pasting information, web scraping tools and libraries allow you to gather data programmatically. \r\n\r\n- **Example**: If you want to collect product prices from an e-commerce website, you can use web scraping to fetch this data instead of manually looking up each product.\r\n\r\n**How Web Scraping Works**:\r\n1. **Send a Request**: The scraper sends a request to the target website to fetch the HTML content.\r\n2. **Parse HTML**: The HTML content is parsed using tools like BeautifulSoup.\r\n3. **Extract Data**: The desired data is extracted from the HTML tags.\r\n4. **Store Data**: The extracted data is then stored in a structured format (e.g., CSV, Excel, JSON, daabase).\r\n\r\n#### **1.2 Why Use Web Scraping?**\r\n\r\nWeb scraping is useful for various purposes, such as:\r\n\r\n- **Data Collection**: Gathering large datasets for analysis (e.g., stock prices, news articles, reviews).\r\n- **Price Monitoring**: Tracking product prices for e-commerce websites.\r\n- **Competitor Analysis**: Monitoring competitors' websites to analyze their products and pricing strategies.\r\n- **Content Aggregation**: Collecting data from multiple sources to provide a unified view (e.g., nws aggregators).\r\n\r\n#### **1.3 Legal Considerations and Best Practices**\r\n\r\nBefore starting web scraping, it is essential to understand the legal and ethical considerations:\r\n\r\n1. **Respect `robots.txt`**: \r\n   - Websites often provide a `robots.txt` file that specifies which pages can or cannot be scraped.\r\n   - Always check and respect the rules defined in this file.\r\n   - Example: `https://example.com/robots.txt`\r\n\r\n2. **Avoid Overloading the Server**:\r\n   - Sending too many requests in a short time can overload the server, causing it to block your IP.\r\n   - Use time intervals (e.g., `time.sleep()`) between requests to avoid being flagged as a bot.\r\n\r\n3. **Do Not Scrape Personal Information**:\r\n   - Avoid scraping sensitive or personal data to comply with privacy regulations like GDPR.\r\n\r\n4. **Check the Website's Terms of Service**:\r\n   - Always review a website’s terms of srvice to ensure scraping is allowed.\r\n\r\n#### **1.4 Setting Up the Environment**\r\n\r\nTo get started with web scraping using BeautifulSoup, you need to set up your environment.\r\n\r\n**Step 1: Installing Python**\r\n- BeautifulSoup is a Python library, so you need Python installed on your system.\r\n- **Download Python**:\r\n  - Go to [Python's official website](https://www.python.org/downloads/) and download the latest version.\r\n- **Check Python Installation**:\r\n  ```bash\r\n  python --version\r\n  ```\r\n\r\n**Step 2: Installing BeautifulSoup**\r\n- BeautifulSoup can be installed using Python's package manager `pip`.\r\n- **Install BeautifulSoup**:\r\n  ```bash\r\n  pip install beautifulsoup4\r\n  ```\r\n\r\n**Step 3: Installing Required Libraries**\r\n1. **Requests Library**:\r\n   - Used for sending HTTP requests to fetch web pages.\r\n   ```bash\r\n   pip install requests\r\n   ```\r\n2. **lxml Parser**:\r\n   - An optional parser that can be used with BeautifulSoup for faster parsing.\r\n   ```bash\r\n   pip install lxml\r\n   ```\r\n\r\n**Step 4: Setting Up a Basic Project Structure**\r\n- **Create a Project lder**:\r\n  ```bash\r\n  mkdir web_scraping_project\r\n  cd web_scraping_proje\n\n\n  ## **2. Getting Started with BeautifulSoup**\r\n\r\nIn this section, we will cover the basics of **BeautifulSoup**, a Python library used for parsing HTML and XML documents. You'll learn how to set it up, create a soup object, use different parsers, and navigate through the parse tree.\r\n\r\n### **Introduction to BeautifulSoup**\r\n\r\n**BeautifulSoup** is a popular Python library for web scraping, which allows us to extract data from HTML and XML documents. It helps to parse and navigate through the HTML content, making it easier to extract the required information.\r\n\r\n**Key Features of BeautifulSoup:**\r\n- Provides tools for parsing HTML/XML documents\r\n- Easy to navigate through the document tree\r\n- Works well with `requests` for making HTTP requests\r\n- Supports multiple parsers (like `html.parser`, `lxml`, etc.)\r\n\r\n### **Basic Syntax and Setup**\r\n\r\n#### **Installation**\r\nTo get started, you need to install the `beautifulsoup4` library and the `lxml` parser. Use the fol\r\npip install requests\r\n```\r\n\r\n#### **Importing Libraries**\r\n\r\n```python\r\nfrom bs4 import BeautifulSoup\r\nimport requests\r\n```\r\n\r\n### **Creating a Soup Object**\r\n\r\nA **Soup Object** is the main object in Beautany specific questions?. Would you like to proceed with **\"Getting Started with BeautifulSoup\"** or explore a specific topic further?specific data.\r\n\r\nWould you like to proceed to the next section, or do you have specific questions about this introduction?","metadata":{}},{"cell_type":"code","source":"from bs4 import BeautifulSoup\nimport requests\n\n\nhtml_content = \"\"\"\n<!DOCTYPE html>\n<html>\n<head>\n    <title>Test Page</title>\n</head>\n<body>\n    <h1>Welcome to BeautifulSoup Tutorial</h1>\n    <p class=\"description\">This is a sample paragraph.</p>\n    <a href=\"https://example.com\" id=\"link1\">Example Link</a>\n</body>\n</html>\n\"\"\"\n\n# Create a BeautifulSoup object\nsoup = BeautifulSoup(html_content, \"html.parser\")\n\n# Output the parsed HTML\nprint(soup.prettify())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T06:03:15.084613Z","iopub.execute_input":"2024-11-19T06:03:15.085090Z","iopub.status.idle":"2024-11-19T06:03:15.372297Z","shell.execute_reply.started":"2024-11-19T06:03:15.085054Z","shell.execute_reply":"2024-11-19T06:03:15.370939Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Using HTML/XML Parsers**\n\nBeautifulSoup supports multiple parsers:\n1. **`html.parser`** - Built-in Python HTML parser.\n2. **`lxml`** - Faster, more lenient parser. Requires `lxml` to be installed.\n3. **`html5lib`** - Parses HTML similar to web browsers.\n\n#### **Example: Using Different Parsers**\n\n\n\n\n\n\n","metadata":{}},{"cell_type":"code","source":"# Using html.parser\nsoup_html = BeautifulSoup(html_content, \"html.parser\")\n\n# Using lxml parser\nsoup_lxml = BeautifulSoup(html_content, \"lxml\")\n\nprint(soup_html.title)\nprint(soup_lxml.title)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T16:54:55.486458Z","iopub.execute_input":"2024-11-18T16:54:55.487016Z","iopub.status.idle":"2024-11-18T16:54:55.498260Z","shell.execute_reply.started":"2024-11-18T16:54:55.486978Z","shell.execute_reply":"2024-11-18T16:54:55.497290Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Parsing a Simple HTML Document**\n\nLet's parse a basic HTML document to extract elements like the title, header, and paragraph.\n\n\n","metadata":{}},{"cell_type":"code","source":"# Extract the title of the page\npage_title = soup.title.text\nprint(\"Page Title:\", page_title)\n\n# Extract the first header\nheader = soup.h1.text\nprint(\"Header:\", header)\n\n# Extract the paragraph text\nparagraph = soup.find('p').text\nprint(\"Paragraph:\", paragraph)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T16:54:56.229526Z","iopub.execute_input":"2024-11-18T16:54:56.230537Z","iopub.status.idle":"2024-11-18T16:54:56.236789Z","shell.execute_reply.started":"2024-11-18T16:54:56.230496Z","shell.execute_reply":"2024-11-18T16:54:56.235682Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n\n### **Navigating the Parse Tree**\n\nNavigating the parse tree involves moving through different tags to access specific data. We use the following attributes:\n\n1. **`.tagName`** - Access a specific tag.\n2. **`.attrs`** - Access the attributes of a tag as a dictionary.\n3. **`.text`** - Extract the text content of a tag.\n\n#### **Example: Accessing Tags, Attributes, and Text**\n","metadata":{}},{"cell_type":"code","source":"# Access the <a> tag\nlink_tag = soup.a\nprint(\"Link Tag:\", link_tag)\n\n# Accessing attributes of the <a> tag\nlink_href = link_tag['href']\nprint(\"Link Href:\", link_href)\n\n# Accessing the text inside the <a> tag\nlink_text = link_tag.text\nprint(\"Link Text:\", link_text)\n\n# Access all attributes of the <a> tag\nlink_attributes = link_tag.attrs\nprint(\"Link Attributes:\", link_attributes)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T16:54:56.322083Z","iopub.execute_input":"2024-11-18T16:54:56.322987Z","iopub.status.idle":"2024-11-18T16:54:56.329723Z","shell.execute_reply.started":"2024-11-18T16:54:56.322949Z","shell.execute_reply":"2024-11-18T16:54:56.328588Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **3. Searching and Navigating with BeautifulSoup**\r\n\r\nNow that you’ve learned the basics of creating a BeautifulSoup object and parsing HTML, let’s dive into searching and navigating the document tree. BeautifulSoup provides several methods to search for elements within the HTML and navigate the parse tree efficiently---\n\r\n#### **3.1. Finding Elements by Tag Name**\r\n\r\nThe most fundamental way to find elements in BeautifulSoup is by searching for them by their tag name. You can use the following methos:\r\n\r\n##### **find()**\r\n\r\nThe `find()` method is used to search for the first occurrence of a tag in the document. It returns a **single element** (the first match) or `None` if no element is found.\r\n\r\n**Syntax:**\r\n```python\r\nsoup.find('tag_name')\r\n`('img')\r\nfor image in images:\r\n    print(image['src'])\r\n```\r\n\r\n**Output:**\r\n```\r\nimage1.jpg\r\nimage2.jpg\r\n```\r\n\r\n---\r\n\r\nThat concludes **Section 3: Searching and Navigating with BeautifulSoup**. Let me know if you need any more details or would like to continue with the next section!","metadata":{}},{"cell_type":"code","source":"from bs4 import BeautifulSoup\n\nhtml_doc = \"<html><head><title>Test Page</title></head><body><h1>Welcome to the test page</h1></body></html>\"\nsoup = BeautifulSoup(html_doc, 'html.parser')\n\n# Find the first <h1> tag\nh1_tag = soup.find('h1')\nprint(h1_tag.text)  # Output: Welcome to the test page","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T16:54:56.440103Z","iopub.execute_input":"2024-11-18T16:54:56.440715Z","iopub.status.idle":"2024-11-18T16:54:56.450953Z","shell.execute_reply.started":"2024-11-18T16:54:56.440652Z","shell.execute_reply":"2024-11-18T16:54:56.449739Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **find_all()**\n\nThe `find_all()` method is used to find all occurrences of a tag. It returns a **list of matching elements** (could be empty if no match is found).\n\n**Syntax:**\n```python\nsoup.find_all('tag_name')\n```\n\n**Example:**","metadata":{}},{"cell_type":"code","source":"html_doc = \"<html><body><h1>Title 1</h1><h1>Title 2</h1><h1>Title 3</h1></body></html>\"\nsoup = BeautifulSoup(html_doc, 'html.parser')\n\n# Find all <h1> tags\nh1_tags = soup.find_all('h1')\nfor tag in h1_tags:\n    print(tag.text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T16:54:56.517377Z","iopub.execute_input":"2024-11-18T16:54:56.517760Z","iopub.status.idle":"2024-11-18T16:54:56.524639Z","shell.execute_reply.started":"2024-11-18T16:54:56.517725Z","shell.execute_reply":"2024-11-18T16:54:56.523264Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n### **3.2. CSS Selectors**\n\nBeautifulSoup allows you to select elements using CSS selectors with the `select()` method. This method gives you more flexibility, especially when working with classes, ids, or nested elements.\n\n#### **Using `select()`**\n\nThe `select()` method allows you to use CSS selectors to find elements. It returns a **list** of elements matching the selector.\n\n**Syntax:**\n```python\nsoup.select('selector')\n```\n\n**Example:**","metadata":{}},{"cell_type":"code","source":"html_doc = '''\n<html>\n    <body>\n        <div class=\"content\">\n            <h1 class=\"header\">Header 1</h1>\n            <p class=\"paragraph\">This is the first paragraph.</p>\n        </div>\n        <div class=\"content\">\n            <h1 class=\"header\">Header 2</h1>\n            <p class=\"paragraph\">This is the second paragraph.</p>\n        </div>\n    </body>\n</html>\n'''\n\nsoup = BeautifulSoup(html_doc, 'html.parser')\n\n# Select all elements with the class 'header'\nheaders = soup.select('.header')\nfor header in headers:\n    print(header.text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T16:54:56.639276Z","iopub.execute_input":"2024-11-18T16:54:56.639626Z","iopub.status.idle":"2024-11-18T16:54:56.647744Z","shell.execute_reply.started":"2024-11-18T16:54:56.639595Z","shell.execute_reply":"2024-11-18T16:54:56.646542Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n#### **Using Filters and Regular Expressions**\n\nYou can use filters or regular expressions with `find_all()` and `select()` to make more specific searches.\n\n**Example using regular expressions:**","metadata":{}},{"cell_type":"code","source":"import requests\n\nhtml_doc = \"<html><body><h1>Title 1</h1><h2>Title 2</h2><h3>Title 3</h3></body></html>\"\nsoup = BeautifulSoup(html_doc, 'html.parser')\n\n# Find all header tags (h1, h2, h3, etc.)\nheaders = soup.find_all(re.compile('h[1-3]'))\nfor header in headers:\n    print(header.text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T05:06:24.139441Z","iopub.execute_input":"2024-11-18T05:06:24.139860Z","iopub.status.idle":"2024-11-18T05:06:24.146919Z","shell.execute_reply.started":"2024-11-18T05:06:24.139819Z","shell.execute_reply":"2024-11-18T05:06:24.145796Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **3.3. Searching with Attributes**\n\nYou can filter elements based on their attributes like `id`, `class`, `href`, etc. These attributes can be passed directly into `find()` or `find_all()`.\n\n#### **Using `class_` (to avoid conflict with Python's `class` keyword)**\nYou can search for elements based on their `class` attribute by passing `class_` to the method.\n\n**Example:**","metadata":{}},{"cell_type":"code","source":"html_doc = \"<html><body><div class='content'>Some content</div><div class='footer'>Footer content</div></body></html>\"\nsoup = BeautifulSoup(html_doc, 'html.parser')\n\n# Find the first div with class 'content'\ncontent_div = soup.find('div', class_='content')\nprint(content_div.text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T05:06:24.549366Z","iopub.execute_input":"2024-11-18T05:06:24.549783Z","iopub.status.idle":"2024-11-18T05:06:24.556436Z","shell.execute_reply.started":"2024-11-18T05:06:24.549743Z","shell.execute_reply":"2024-11-18T05:06:24.555226Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **Using `id`**\nYou can search by `id` attribute similarly.\n\n**Example:**","metadata":{}},{"cell_type":"code","source":"html_doc = \"<html><body><div id='main'>Main content</div></body></html>\"\nsoup = BeautifulSoup(html_doc, 'html.parser')\n\n# Find the div with the id 'main'\nmain_div = soup.find('div', id='main')\nprint(main_div.text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T05:06:25.315625Z","iopub.execute_input":"2024-11-18T05:06:25.316598Z","iopub.status.idle":"2024-11-18T05:06:25.322874Z","shell.execute_reply.started":"2024-11-18T05:06:25.316533Z","shell.execute_reply":"2024-11-18T05:06:25.321757Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **Using other attributes**\nYou can also search by other attributes like `href`, `src`, etc.\n\n\n**Example:**","metadata":{}},{"cell_type":"code","source":"html_doc = \"<html><body><a href='https://example.com'>Visit Example</a></body></html>\"\nsoup = BeautifulSoup(html_doc, 'html.parser')\n\n# Find the anchor tag with a specific href\nlink = soup.find('a', href='https://example.com')\nprint(link.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T05:06:26.142597Z","iopub.execute_input":"2024-11-18T05:06:26.142984Z","iopub.status.idle":"2024-11-18T05:06:26.149564Z","shell.execute_reply.started":"2024-11-18T05:06:26.142948Z","shell.execute_reply":"2024-11-18T05:06:26.148284Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n### **3.4. Navigating the Tree**\n\nOnce you have found elements, you can navigate the document tree to explore relationships between elements.\n\n#### **Using `parent`**\nThis allows you to access the parent element of the current tag.\n\n**Example:**","metadata":{}},{"cell_type":"code","source":"html_doc = \"<html><body><h1>Welcome</h1></body></html>\"\nsoup = BeautifulSoup(html_doc, 'html.parser')\n\n# Find the <h1> tag and get its parent\nh1_tag = soup.find('h1')\nparent = h1_tag.parent\nprint(parent.name)  \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T05:06:26.700630Z","iopub.execute_input":"2024-11-18T05:06:26.701519Z","iopub.status.idle":"2024-11-18T05:06:26.707355Z","shell.execute_reply.started":"2024-11-18T05:06:26.701474Z","shell.execute_reply":"2024-11-18T05:06:26.706218Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **Using `children`**\nThe `children` attribute allows you to access all child elements of the current tag.\n\n**Example:**\n","metadata":{}},{"cell_type":"code","source":"html_doc = \"<html><body><h1>Welcome</h1><p>Content</p></body></html>\"\nsoup = BeautifulSoup(html_doc, 'html.parser')\n\n# Get all children of the <body> tag\nbody_tag = soup.find('body')\nfor child in body_tag.children:\n    print(child)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T05:06:26.787422Z","iopub.execute_input":"2024-11-18T05:06:26.787824Z","iopub.status.idle":"2024-11-18T05:06:26.794331Z","shell.execute_reply.started":"2024-11-18T05:06:26.787786Z","shell.execute_reply":"2024-11-18T05:06:26.793233Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **Using `next_sibling` and `previous_sibling`**\nThese attributes let you move between siblings of an element.\n\n**Example:**","metadata":{}},{"cell_type":"code","source":"html_doc = \"<html><body><h1>Header 1</h1><h2>Header 2</h2></body></html>\"\nsoup = BeautifulSoup(html_doc, 'html.parser')\n\n# Get the next sibling of <h1>\nheader1 = soup.find('h1')\nnext_header = header1.find_next_sibling()\nprint(next_header.text)  # Output: Header 2\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T05:06:26.868887Z","iopub.execute_input":"2024-11-18T05:06:26.870026Z","iopub.status.idle":"2024-11-18T05:06:26.876157Z","shell.execute_reply.started":"2024-11-18T05:06:26.869978Z","shell.execute_reply":"2024-11-18T05:06:26.874842Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **3.5. Getting All Links and Images from a Page**\n\nBeautifulSoup makes it easy to get all links (`<a>` tags) and images (`<img>` tags) from a page.\n\n#### **Extracting All Links**\nTo get all links from a page, you can search for all anchor (`<a>`) tags and extract the `href` attribute.\n\n**Example:**\n","metadata":{}},{"cell_type":"code","source":"html_doc = \"<html><body><a href='https://example1.com'>Link 1</a><a href='https://example2.com'>Link 2</a></body></html>\"\nsoup = BeautifulSoup(html_doc, 'html.parser')\n\n# Find all links\nlinks = soup.find_all('a')\nfor link in links:\n    print(link['href'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T05:06:26.944309Z","iopub.execute_input":"2024-11-18T05:06:26.944996Z","iopub.status.idle":"2024-11-18T05:06:26.951288Z","shell.execute_reply.started":"2024-11-18T05:06:26.944952Z","shell.execute_reply":"2024-11-18T05:06:26.950277Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **Extracting All Images**\nSimilarly, you can extract all images by looking for `<img>` tags and accessing the `src` attribute.\n\n**Example:**\n","metadata":{}},{"cell_type":"code","source":"html_doc = \"<html><body><img src='image1.jpg' alt='Image 1'><img src='image2.jpg' alt='Image 2'></body></html>\"\nsoup = BeautifulSoup(html_doc, 'html.parser')\n\n# Find all images\nimages = soup.find_all('img')\nfor image in images:\n    print(image['src'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T05:06:27.026658Z","iopub.execute_input":"2024-11-18T05:06:27.027474Z","iopub.status.idle":"2024-11-18T05:06:27.033848Z","shell.execute_reply.started":"2024-11-18T05:06:27.027431Z","shell.execute_reply":"2024-11-18T05:06:27.032696Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **4. Extracting Data with BeautifulSoup**\r\n\r\nIn this section, we will cover how to extract data from HTML documents using BeautifulSoup. You'll learn how to extract text from tags, get attributes from tags, and deal with HTML tables. We'll also dive into handling nested tags and missing data.\r\n\n\r\n#### **4.1 Extracting Text from Tags**\r\n\r\nThe primary purpose of web scraping is often to extract the data from the HTML tags. BeautifulSoup makes it very easy to retrieve the text content from tags.\r\n\r\n**Example:**\r\n","metadata":{}},{"cell_type":"code","source":"from bs4 import BeautifulSoup\n\nhtml_doc = \"\"\"\n<html>\n  <head><title>Sample Page</title></head>\n  <body>\n    <h1>Welcome to Web Scraping</h1>\n    <p>This is a simple HTML page for testing.</p>\n  </body>\n</html>\n\"\"\"\n\nsoup = BeautifulSoup(html_doc, 'html.parser')\n\n# Extracting the title of the page\ntitle = soup.title.string\nprint(\"Title:\", title)  # Output: Sample Page\n\n# Extracting text from an h1 tag\nheader = soup.h1.text\nprint(\"Header:\", header)  # Output: Welcome to Web Scraping\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T05:06:27.761173Z","iopub.execute_input":"2024-11-18T05:06:27.761622Z","iopub.status.idle":"2024-11-18T05:06:27.768831Z","shell.execute_reply.started":"2024-11-18T05:06:27.761556Z","shell.execute_reply":"2024-11-18T05:06:27.767739Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"In the above code, we used `.string` to get the contents of the `title` tag and `.text` to get the text inside the `h1` tag.\r\n\r\n### **4.2 Using the `get()` Method for Attributes**\r\n\r\nSometimes you need to extract attributes (e.g., `href`, `src`, `alt`) from tags. This can be done using the `.get()` method.\r\n\r\n**Exa*","metadata":{}},{"cell_type":"code","source":"html_doc = \"\"\"\n<html>\n  <body>\n    <a href=\"https://www.example.com\" target=\"_blank\">Click here</a>\n  </body>\n</html>\n\"\"\"\n\nsoup = BeautifulSoup(html_doc, 'html.parser')\n\n# Extracting the href attribute from the <a> tag\nlink = soup.a.get('href')\nprint(\"Link:\", link)  # Output: https://www.example.com\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T05:06:28.791096Z","iopub.execute_input":"2024-11-18T05:06:28.791507Z","iopub.status.idle":"2024-11-18T05:06:28.797843Z","shell.execute_reply.started":"2024-11-18T05:06:28.791468Z","shell.execute_reply":"2024-11-18T05:06:28.796786Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **4.3 Extracting Tables**\r\n\r\nWebsites often present data in HTML tables. Extracting table data with BeautifulSoup involves finding the `table` tag and extracting its content row by row.\r\n\n##### **Finding and Extracting Table Data**\r\n\r\nYou can find a table using `soup.find()` or `soup.find_all()`. After finding the table, you can extract rows and columns to get the actual data.\r\n\r\n**Example:**\r\n\r\n","metadata":{}},{"cell_type":"code","source":"html_doc = \"\"\"\n<html>\n  <body>\n    <table>\n      <tr><th>Name</th><th>Age</th></tr>\n      <tr><td>Alice</td><td>24</td></tr>\n      <tr><td>Bob</td><td>30</td></tr>\n    </table>\n  </body>\n</html>\n\"\"\"\n\nsoup = BeautifulSoup(html_doc, 'html.parser')\n\n# Extracting the table\ntable = soup.find('table')\n\n# Extracting rows\nrows = table.find_all('tr')\n\nfor row in rows:\n    columns = row.find_all('td')\n    if columns:  # Skip headers\n        name = columns[0].text\n        age = columns[1].text\n        print(f\"Name: {name}, Age: {age}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T05:06:31.942969Z","iopub.execute_input":"2024-11-18T05:06:31.943388Z","iopub.status.idle":"2024-11-18T05:06:31.951323Z","shell.execute_reply.started":"2024-11-18T05:06:31.943336Z","shell.execute_reply":"2024-11-18T05:06:31.950216Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\r\nIn this example, we use `find_all('tr')` to get all the rows, and within each row, we extract the `td` elements for data.\r\n\n##### **Parsing HTML Tables**\r\n\r\nIn more complex tables, you may have to work with nested tables or mixed tag structures. You can still parse them similarly by navigating through nested tags and applying filters as ne\n\n\r---\r\n\r\n#### **4.4 Extracting Data from Nested Tags**\r\n\r\nHTML structures can be complex, with tags nested inside other tags. BeautifulSoup allows you to access nested tags easily.\r\n\r\n**Example:**\r\n","metadata":{}},{"cell_type":"code","source":"html_doc = \"\"\"\n<html>\n  <body>\n    <div class=\"content\">\n      <h2>Article Title</h2>\n      <p>Some content here...</p>\n      <a href=\"https://example.com\">Read more</a>\n    </div>\n    <div class=\"content\">\n      <h2>Another Article</h2>\n      <p>Different content here...</p>\n      <a href=\"https://another.com\">Read more</a>\n    </div>\n  </body>\n</html>\n\"\"\"\n\nsoup = BeautifulSoup(html_doc, 'html.parser')\n\n# Extracting all divs with the class \"content\"\ncontents = soup.find_all('div', class_='content')\n\nfor content in contents:\n    # Extracting the article title and link\n    title = content.h2.text\n    link = content.a['href']\n    print(f\"Title: {title}, Link: {link}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T05:06:32.029280Z","iopub.execute_input":"2024-11-18T05:06:32.029743Z","iopub.status.idle":"2024-11-18T05:06:32.037912Z","shell.execute_reply.started":"2024-11-18T05:06:32.029699Z","shell.execute_reply":"2024-11-18T05:06:32.036746Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\r\nHere, the `.find_all()` method is used to find all divs with the class `content`. Then, we extract the nested `h2` (title) and `a` (link) tags from each div.\r\n\r\n---\r\r\n#### **4.5 Handling Missing or Incomplete Data**\r\n\r\nWhen scraping data, sometimes you may encounter missing or incomplete values, such as empty cells or broken links. You can handle this by checking if the tag exists before extracting its content.\r\n\r\n**Example:**\r\n","metadata":{}},{"cell_type":"code","source":"html_doc = \"\"\"\n<html>\n  <body>\n    <ul>\n      <li>Item 1</li>\n      <li></li>  <!-- Empty item -->\n      <li>Item 3</li>\n    </ul>\n  </body>\n</html>\n\"\"\"\n\nsoup = BeautifulSoup(html_doc, 'html.parser')\n\n# Extracting all items, skipping empty ones\nitems = soup.find_all('li')\n\nfor item in items:\n    text = item.text.strip()\n    if text:  # Checking if the item is not empty\n        print(f\"Item: {text}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T05:06:32.105194Z","iopub.execute_input":"2024-11-18T05:06:32.105651Z","iopub.status.idle":"2024-11-18T05:06:32.113681Z","shell.execute_reply.started":"2024-11-18T05:06:32.105603Z","shell.execute_reply":"2024-11-18T05:06:32.112030Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"base_url = \"https://example.com/products?page=\"\nfor page_num in range(1, 6):  # Scraping first 5 pages\n    url = base_url + str(page_num)\n    response = requests.get(url)\n    soup = BeautifulSoup(response.content, 'html.parser')\n    soup.body\n    # Extract data here\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T05:06:32.158313Z","iopub.execute_input":"2024-11-18T05:06:32.158756Z","iopub.status.idle":"2024-11-18T05:06:34.694210Z","shell.execute_reply.started":"2024-11-18T05:06:32.158716Z","shell.execute_reply":"2024-11-18T05:06:34.693273Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"In this case, we use `.strip()` to remove any leading or trailing spaces and check if the item has any text. If it’s empty, we skip it.\r\n\r\n---\r\n\r\n### **Summary of Section 4**\r\n- **Extracting Text**: `.text` or `.string` is used to extract the text content from tags.\r\n- **Extracting Attributes**: `.get('attribute')` helps to fetch tag attributes like `href`, `src`, etc.\r\n- **Extracting Tables**: Use `find_all('tr')` and `find_all('td')` to extract table rows and columns.\r\n- **Handling Nested Tags**: You can navigate through deeply nested tags with `find()` and `find_all()`.\r\n- **Handling Missing Data**: Ensure robustness by checking if the tag contains data befor cce\n\n\n\r\n### **5. Advanced HTML Parsing Techques**\r\n\r\n#### **5.1 Working with Complex HTML Structures**\r\n   - When scraping complex HTML structures, BeautifulSoup provides a way to navigate nested tags and extract relevant data.\r\n   - Complex HTML often includes nested elements like `<div>`, `<span>`, `<ul>`, `<li>`, etc. It's essential to identify the tags that encapsulate the data you need.\r\n   - Example: Scraping blog post content or product listings with multiple nested levels.\r\n   \r\n   **Steps:**\r\n   - Identify parent elements (e.g., `<div class=\"post\">`) and drill down to child elements.\r\n   - Use `.find()` and `.find_all()` to search for specific tags inside the\n\n **Tip:** When working with deeply nested structures, consider using CSS selectors or XPath for easier targeting.\nse parent elemee:\r\nxample:\r\nssing it.\r\n","metadata":{}},{"cell_type":"code","source":"import requests\nfrom bs4 import BeautifulSoup\n\nurl = 'https://quotes.toscrape.com/'  \n\nresponse = requests.get(url)\nhtml_content = response.text\n\nsoup = BeautifulSoup(html_content, 'html.parser')\n\npost = soup.find('div', class_='quote')  \nquote = post.find('span', class_='text').text \n\nprint(f\"Quote: {quote}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T05:06:40.065121Z","iopub.execute_input":"2024-11-18T05:06:40.065535Z","iopub.status.idle":"2024-11-18T05:06:40.732025Z","shell.execute_reply.started":"2024-11-18T05:06:40.065497Z","shell.execute_reply":"2024-11-18T05:06:40.730823Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\r\n### **5.2 Handling Pagination in Web Scraping**\r\n   - Pagination occurs when a website splits content across multiple pages. You need to extract data from all pages to get the full set of information.\r\n   - Example: Scraping product listings from an e-commerce website that has multiple pages.\r\n   \r\n   **Steps:**\r\n   - Identify the pattern in the URL for the pages (e.g., `page=1`, `page=2`).\r\n   - Loop through each page by modifying the URL and scraping th\n\n **Tip:** Check if the website uses a “next” button, and ensure to handle edge cases (e.g., last page).e\n   ta.\r\n   \r\n   **Example of pagination:**\r\n","metadata":{}},{"cell_type":"code","source":"base_url = \"https://dummyjson.com/products/\"\nfor page_num in range(1, 6):  # Scraping first 5 pages\n    url = base_url + str(page_num)\n    print(url)\n    response = requests.get(url)\n    soup = BeautifulSoup(response.content, 'html.parser')\n    # Extract data here\n    print(soup.body)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T05:06:40.964319Z","iopub.execute_input":"2024-11-18T05:06:40.964746Z","iopub.status.idle":"2024-11-18T05:06:43.745073Z","shell.execute_reply.started":"2024-11-18T05:06:40.964705Z","shell.execute_reply":"2024-11-18T05:06:43.744035Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **5.3 Dealing with JavaScript-Rendered Content**\r\n   - Many modern websites use JavaScript to load content dynamically after the page is loaded. BeautifulSoup alone won't be able to scrape data from these pages because it only works with static HTML.\r\n   - To scrape JavaScript-generated content, you need to either:\r\n     - Use a tool like **Selenium** or **Playwright** to render JavaScript and extract the final HTML.\r\n     - Look for an API call in the network traffic that provides the data in JSON or XML format\n\r\n#### **5.4 Using Selenium for Dynamic Content**\r\n   - **Selenium** automates a real browser (e.g., Chrome or Firefox) to render pages, execute JavaScript, and allow you to scrape the content after the page loads fully.\r\n   \r\n   **Steps:**\r\n   1. Install Selenium and a WebDriver (e.g., ChromeDriver for Chrome).\r\n   2. Set up Selenium to navigate to a webpage and extract data.\r\n   \r\n   **Example:**\r\n","metadata":{}},{"cell_type":"code","source":"pip install selenium","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T05:06:46.685388Z","iopub.execute_input":"2024-11-18T05:06:46.686230Z","iopub.status.idle":"2024-11-18T05:07:02.319793Z","shell.execute_reply.started":"2024-11-18T05:06:46.686184Z","shell.execute_reply":"2024-11-18T05:07:02.318511Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.common.keys import Keys\n\n# Set up Chrome WebDriver\ndriver = webdriver.Chrome(executable_path='/path/to/chromedriver')\ndriver.get(\"https://example.com\")\n\n# Wait for the page to load and scrape the content\ncontent = driver.find_element(By.CLASS_NAME, 'content-class').text\nprint(content)\n\ndriver.quit()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"   - **Tip:** Use `WebDriverWait` to wait for elements to load dynamically.\r\n\r\n### **5.5 Combining BeautifulSoup with Selenium**\r\n   - Sometimes, you may need the power of both tools: **Selenium** for rendering dynamic pages and **BeautifulSoup** for parsing the HTML after it's fully rendered.\r\n   \r\n   **Steps:**\r\n   - Use Selenium to navigate and render the page.\r\n   - Use BeautifulSoup to parse the rendered HTML.\r\n\r\n   **Example:**\r\n","metadata":{}},{"cell_type":"code","source":"from selenium import webdriver\nfrom bs4 import BeautifulSoup\n\ndriver = webdriver.Chrome(executable_path='/path/to/chromedriver')\ndriver.get(\"https://example.com\")\n\n# Get page source after rendering JavaScript\npage_source = driver.page_source\n\n# Parse with BeautifulSoup\nsoup = BeautifulSoup(page_source, 'html.parser')\ndata = soup.find('div', class_='product').text\n\ndriver.quit()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"   - **Tip:** This method is particularly useful for scraping data from websites that load content dynamically via JavaScript.\r\n\r#### **5.6 Handling AJAX Requests**\r\n   - Some websites load content using **AJAX** (Asynchronous JavaScript and XML) requests. These requests fetch data from the server without refreshing the page.\r\n   - BeautifulSoup cannot directly handle AJAX requests because it only parses the static HTML, so you need to inspect network traffic and make similar requests to extract the data.\r\n   \r\n   **Steps:**\r\n   1. Open the website in a browser and use the **Network** tab in the developer tools to identify AJAX requests.\r\n   2. Extract the URL of the AJAX request (usually a **JSON** or **XML** response).\r\n   3. Use Python’s `requests` library to make a similar request.\r\n   \r\n   **Example (Extracting JSON via AJAX):**\r\n","metadata":{}},{"cell_type":"code","source":"import requests\n\nurl = 'https://example.com/ajax/data'\nresponse = requests.get(url)\ndata = response.json()  # Assuming the response is JSON\nprint(data)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"   - **Tip:** Make sure to pass any necessary headers (like `User-Agent`) to mimic a browser request.\r\n\r\n### **5.7 Using Network Tab to Identify Requests**\r\n   - The **Network Tab** in the browser's developer tools allows you to view network requests made by the webpage, including AJAX calls, image requests, and scripts.\r\n   - You can inspect these requests to find the ones that contain the data you're interested in.\r\n   \r\n   **Steps:**\r\n   1. Open the website in a browser (e.g., Chrome).\r\n   2. Open the **Developer Tools** (F12 or right-click and select \"Inspect\").\r\n   3. Go to the **Network** tab and reload the page.\r\n   4. Look for XHR (XMLHttpRequest) or Fetch requests that contain the data in JSON or XML format.\r\n   5. Copy the request URL or observe the parameters (e.g., pagination or filters) and replicate the request us\n\n   - **Tip:** You can often find APIs that provide structured data directly, avoiding the need for scraping HTML.ing Python.\r\n\r\n   **Example:**\r\n","metadata":{}},{"cell_type":"code","source":"import requests\n\n# Replace with the actual AJAX URL you found\nurl = 'https://jsonplaceholder.typicode.com/posts'  # Example URL\n\n# Optional: Add headers to mimic a browser request\nheaders = {\n    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36'\n}\n\nresponse = requests.get(url, headers=headers)\ndata = response.json()  # Assuming the response is JSON\nprint(data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T05:18:28.243541Z","iopub.execute_input":"2024-11-18T05:18:28.244406Z","iopub.status.idle":"2024-11-18T05:18:28.293502Z","shell.execute_reply.started":"2024-11-18T05:18:28.244351Z","shell.execute_reply":"2024-11-18T05:18:28.292130Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **6. Data Cleaning and Manipulation**\r\n\r\nWhen working with raw data extracted through web scraping, it often contains noise like HTML tags, special characters, extra spaces, etc. We need to clean and process this data to make it suitable for analysis. Let's cover the main techniques for data cleaning and manipulation.\r\n\r#### **6.1 Cleaning Extracted Data**\r\n\r\nAfter scraping data, it’s crucial to ensure that your dataset is free of HTML tags and any unwanted characters. Here’s an example to demonstrate cleaning scraped data.\r\n\r\n**Example Code:**\r\n","metadata":{}},{"cell_type":"code","source":"from bs4 import BeautifulSoup\nimport requests\nimport re\n\n# Sample URL\nurl = 'https://quotes.toscrape.com/'\n\n# Make a request\nresponse = requests.get(url)\nsoup = BeautifulSoup(response.content, 'html.parser')\n\n# Extract all quotes\nquotes = soup.find_all('span', class_='text')\n\n# Store quotes in a list\ncleaned_quotes = []\n\nfor quote in quotes:\n    raw_text = quote.get_text()  # Extract text\n    cleaned_text = re.sub(r'[^\\w\\s]', '', raw_text)  # Remove punctuation\n    cleaned_quotes.append(cleaned_text)\n\nprint(cleaned_quotes)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T05:21:42.399470Z","iopub.execute_input":"2024-11-18T05:21:42.399929Z","iopub.status.idle":"2024-11-18T05:21:43.036771Z","shell.execute_reply.started":"2024-11-18T05:21:42.399888Z","shell.execute_reply":"2024-11-18T05:21:43.035620Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Explanation:**\r\n- `re.sub(r'[^\\w\\s]', '', raw_text)` is used to remove punctuation.\r\n- `.get_text()` extracts the raw text content, stripping out HTML tags.\n\r\n#### **6.2 Removing HTML Tags and Special Characters**\r\n\r\nWe can also use BeautifulSoup's built-in methods or regex to clean text further.\r\n\r\n**Using BeautifulSoup's Built-in Method:**\r\n","metadata":{}},{"cell_type":"code","source":"for quote in quotes:\n    text = quote.text.strip()  # Removes leading/trailing whitespace\n    print(text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T05:22:13.416688Z","iopub.execute_input":"2024-11-18T05:22:13.417134Z","iopub.status.idle":"2024-11-18T05:22:13.423031Z","shell.execute_reply.started":"2024-11-18T05:22:13.417092Z","shell.execute_reply":"2024-11-18T05:22:13.421896Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Using Regex to Remove Special Characters:**","metadata":{}},{"cell_type":"code","source":"clean_text = re.sub(r'\\s+', ' ', text)  # Replaces multiple spaces with a single space\nprint(clean_text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T05:22:38.771217Z","iopub.execute_input":"2024-11-18T05:22:38.771647Z","iopub.status.idle":"2024-11-18T05:22:38.777202Z","shell.execute_reply.started":"2024-11-18T05:22:38.771610Z","shell.execute_reply":"2024-11-18T05:22:38.776152Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **6.3 Using Python's `re` Library for Text Processing**\r\n\r\nThe `re` (regular expression) library is powerful for pattern matching and text processing.\r\n\r\n**Example - Removing Digits:**\r\n","metadata":{}},{"cell_type":"code","source":"text_with_numbers = \"Quote123 with 456 some 789 numbers\"\ncleaned_text = re.sub(r'\\d+', '', text_with_numbers)  # Removes digits\nprint(cleaned_text) \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T05:23:06.079022Z","iopub.execute_input":"2024-11-18T05:23:06.079442Z","iopub.status.idle":"2024-11-18T05:23:06.085719Z","shell.execute_reply.started":"2024-11-18T05:23:06.079401Z","shell.execute_reply":"2024-11-18T05:23:06.084355Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Example - Extracting Specific Patterns:**","metadata":{}},{"cell_type":"code","source":"emails = \"Contact us at info@example.com or support@example.org\"\nfound_emails = re.findall(r'\\S+@\\S+', emails)\nprint(found_emails) \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T05:23:45.851851Z","iopub.execute_input":"2024-11-18T05:23:45.852391Z","iopub.status.idle":"2024-11-18T05:23:45.858519Z","shell.execute_reply.started":"2024-11-18T05:23:45.852345Z","shell.execute_reply":"2024-11-18T05:23:45.857383Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **6.4 Handling Encoding Issues**\r\n\r\nWhen scraping data from various websites, encoding issues may arise. Handling them effectively is crucial for data quality.\r\n\r\n**Handling Common Encoding Issues:**\r\n","metadata":{}},{"cell_type":"code","source":"response = requests.get(url)\nresponse.encoding = 'utf-8'  # Ensure correct encoding\nsoup = BeautifulSoup(response.content, 'html.parser')\n\n# Example of replacing encoding issues like smart quotes\ncleaned_text = response.text.replace(\"\\u201c\", '\"').replace(\"\\u201d\", '\"')\nprint(cleaned_text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T05:24:19.721557Z","iopub.execute_input":"2024-11-18T05:24:19.722014Z","iopub.status.idle":"2024-11-18T05:24:20.306937Z","shell.execute_reply.started":"2024-11-18T05:24:19.721974Z","shell.execute_reply":"2024-11-18T05:24:20.305816Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **6.5 Using Pandas to Organize Scraped Data**\r\n\r\nThe **Pandas** library can help in organizing and analyzing data effectively.\r\n\r\n**Installing Pandas:**\r\n```bash\r\npip install pandas\r\n```\r\n\r\n**Example - Organizing Data in a DataFrame:**\r\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Example scraped data\ndata = {\n    'Quote': cleaned_quotes,\n    'Author': [author.get_text() for author in soup.find_all('small', class_='author')]\n}\n\n# Create a DataFrame\ndf = pd.DataFrame(data)\n\n# Display the DataFrame\nprint(df.head())\n\n# Save to CSV\ndf.to_csv('quotes.csv', index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T05:26:34.107783Z","iopub.execute_input":"2024-11-18T05:26:34.108229Z","iopub.status.idle":"2024-11-18T05:26:34.132353Z","shell.execute_reply.started":"2024-11-18T05:26:34.108190Z","shell.execute_reply":"2024-11-18T05:26:34.131088Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **7. Exporting and Storing Scraped Data**\r\n\r\nWhen scraping data, it is important to store the extracted data in a usable format for further analysis or reporting. Here’s how you can do it:\r\r\n#### **7.1 Saving Data to CSV Files**\r\n\r\nCSV (Comma Separated Values) is a popular format for data storage because it is simple and widely supported.\r\n\r\n**Example Code: Saving to CSV**\r\n","metadata":{}},{"cell_type":"code","source":"import requests\nfrom bs4 import BeautifulSoup\nimport csv\n\n# URL for example scraping\nurl = \"https://quotes.toscrape.com/page/1/\"\nresponse = requests.get(url)\nsoup = BeautifulSoup(response.content, \"html.parser\")\n\n# Extract quotes and authors\nquotes = soup.find_all(\"span\", class_=\"text\")\nauthors = soup.find_all(\"small\", class_=\"author\")\n\n# Prepare CSV file\nwith open(\"quotes.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as file:\n    writer = csv.writer(file)\n    writer.writerow([\"Quote\", \"Author\"])  # Header\n    for quote, author in zip(quotes, authors):\n        writer.writerow([quote.text, author.text])\n\nprint(\"Data saved to quotes.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T05:29:09.849735Z","iopub.execute_input":"2024-11-18T05:29:09.850168Z","iopub.status.idle":"2024-11-18T05:29:10.494821Z","shell.execute_reply.started":"2024-11-18T05:29:09.850129Z","shell.execute_reply":"2024-11-18T05:29:10.493607Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **7.2 Saving Data to Excel Files**\r\n\r\nFor saving data to Excel files, we use the **Pandas** library, which provides easy methods to export DataFrames.\r\n\r\n**Example Code: Saving to Excel**\r\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Create a DataFrame\ndata = {\n    \"Quote\": [quote.text for quote in quotes],\n    \"Author\": [author.text for author in authors]\n}\ndf = pd.DataFrame(data)\n\n# Save to Excel\ndf.to_excel(\"quotes.xlsx\", index=False)\nprint(\"Data saved to quotes.xlsx\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T05:30:33.871524Z","iopub.execute_input":"2024-11-18T05:30:33.871974Z","iopub.status.idle":"2024-11-18T05:30:34.182049Z","shell.execute_reply.started":"2024-11-18T05:30:33.871934Z","shell.execute_reply":"2024-11-18T05:30:34.180673Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **7.3 Saving Data to JSON Files**\r\n\r\nJSON is a preferred format for storing hierarchical or nested data.\r\n\r\n**Example Code: Saving to JSON**\r\n","metadata":{}},{"cell_type":"code","source":"import json\n\n# Prepare data\nquotes_data = [{\"quote\": quote.text, \"author\": author.text} for quote, author in zip(quotes, authors)]\n\n# Save to JSON\nwith open(\"quotes.json\", \"w\", encoding=\"utf-8\") as file:\n    json.dump(quotes_data, file, ensure_ascii=False, indent=4)\n\nprint(\"Data saved to quotes.json\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T05:31:04.273276Z","iopub.execute_input":"2024-11-18T05:31:04.273721Z","iopub.status.idle":"2024-11-18T05:31:04.281751Z","shell.execute_reply.started":"2024-11-18T05:31:04.273677Z","shell.execute_reply":"2024-11-18T05:31:04.280513Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **7.4 Storing Data in Databases (SQLite, PostgreSQL)**\r\n\r\nFor persistent and large-scale data storage, using a database is a better approach. Here’s an example using SQLite.\r\n\r\n**Setting Up SQLite:**\r\n\r\n```bash\r\npip install sqlite3\r\n```\r\n\r\n**Example Code: Saving to SQLite Database**\r\n","metadata":{}},{"cell_type":"code","source":"import sqlite3\n\n# Connect to SQLite Database (or create it)\nconn = sqlite3.connect(\"quotes.db\")\ncursor = conn.cursor()\n\n# Create table\ncursor.execute('''\nCREATE TABLE IF NOT EXISTS quotes (\n    id INTEGER PRIMARY KEY AUTOINCREMENT,\n    quote TEXT NOT NULL,\n    author TEXT NOT NULL\n)\n''')\n\n# Insert data into table\nfor quote, author in zip(quotes, authors):\n    cursor.execute(\"INSERT INTO quotes (quote, author) VALUES (?, ?)\", (quote.text, author.text))\n\n# Commit changes and close connection\nconn.commit()\nconn.close()\n\nprint(\"Data saved to SQLite database quotes.db\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T05:31:41.172463Z","iopub.execute_input":"2024-11-18T05:31:41.172908Z","iopub.status.idle":"2024-11-18T05:31:41.201533Z","shell.execute_reply.started":"2024-11-18T05:31:41.172870Z","shell.execute_reply":"2024-11-18T05:31:41.200263Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Using PostgreSQL:**\n\nFor PostgreSQL, you need to set up the database first and use the `psycopg2` library:\n\n```bash\npip install psycopg2\n```\n\n**Example Code: Saving to PostgreSQL Database**","metadata":{}},{"cell_type":"code","source":"import psycopg2\n\n# Connect to PostgreSQL\nconn = psycopg2.connect(\n    database=\"your_db\",\n    user=\"your_user\",\n    password=\"your_password\",\n    host=\"localhost\",\n    port=\"5432\"\n)\ncursor = conn.cursor()\n\n# Create table\ncursor.execute('''\nCREATE TABLE IF NOT EXISTS quotes (\n    id SERIAL PRIMARY KEY,\n    quote TEXT NOT NULL,\n    author TEXT NOT NULL\n)\n''')\n\n# Insert data\nfor quote, author in zip(quotes, authors):\n    cursor.execute(\"INSERT INTO quotes (quote, author) VALUES (%s, %s)\", (quote.text, author.text))\n\nconn.commit()\nconn.close()\nprint(\"Data saved to PostgreSQL database\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **7.5 Creating a Simple API for Scraped Data with Flask**\r\n\r\nCreating an API endpoint can be useful to serve your scraped data.\r\n\r\n**Install Flask:**\r\n\r\n```bash\r\npip install flask\r\n```\r\n\r\n**Flask API Example:**\r\n","metadata":{}},{"cell_type":"code","source":"from flask import Flask, jsonify\nimport json\n\napp = Flask(__name__)\n\n# Load scraped data\nwith open(\"quotes.json\", \"r\", encoding=\"utf-8\") as file:\n    data = json.load(file)\n\n@app.route(\"/api/quotes\", methods=[\"GET\"])\ndef get_quotes():\n    return jsonify(data)\n\nif __name__ == \"__main__\":\n    app.run(debug=True)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Run the Flask API:**\r\n\r\n```bash\r\npython app.py\r\n```\r\n\r\n**Access the API:**\r\nOpen `http://127.0.0.1:5000/api/quotes` in your browser to see the data in JSON format.\r\n\r\n---\r\n\r\n### **Additional Practice: Using Other URLs for Data Extraction**\r\n\r\nLet's extend our example to handle multiple pages from another site. Here’s an adjusted approach using an e-commerce website example:\r\n\r\n**Example Code for Handling Multiple Pages:**\r\n","metadata":{}},{"cell_type":"code","source":"import requests\nfrom bs4 import BeautifulSoup\nimport csv\n\n# Define base URL and page range\nbase_url = \"https://books.toscrape.com/catalogue/page-{}.html\"\npage_range = 5  # Number of pages to scrape\n\n# Prepare CSV file\nwith open(\"books.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as file:\n    writer = csv.writer(file)\n    writer.writerow([\"Title\", \"Price\"])\n\n    for page in range(1, page_range + 1):\n        url = base_url.format(page)\n        response = requests.get(url)\n        soup = BeautifulSoup(response.content, \"html.parser\")\n        \n        # Scrape data\n        books = soup.find_all(\"article\", class_=\"product_pod\")\n        for book in books:\n            title = book.h3.a[\"title\"]\n            price = book.find(\"p\", class_=\"price_color\").text\n            writer.writerow([title, price])\n        \n        print(f\"Page {page} scraped successfully!\")\n\nprint(\"Data saved to books.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T05:38:23.847738Z","iopub.execute_input":"2024-11-18T05:38:23.848179Z","iopub.status.idle":"2024-11-18T05:38:28.204150Z","shell.execute_reply.started":"2024-11-18T05:38:23.848129Z","shell.execute_reply":"2024-11-18T05:38:28.203044Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **8. Handling Errors and Exceptions in Web Scraping**\r\n\r\n### **8.1 Common Errors in BeautifulSoup**\r\n\r\nWhile working with BeautifulSoup, you might encounter some common errors such as:\r\n\r\n1. **AttributeError**: Occurs when you try to access an attribute or method that doesn't exist on a BeautifulSoup object. For example, trying to call `.text` on a `None` object.\r\n","metadata":{}},{"cell_type":"code","source":"title = soup.find(\"h1\").text  # If `h1` tag is not found, this raises an AttributeError\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T05:41:24.737463Z","iopub.execute_input":"2024-11-18T05:41:24.738054Z","iopub.status.idle":"2024-11-18T05:41:24.744828Z","shell.execute_reply.started":"2024-11-18T05:41:24.738001Z","shell.execute_reply":"2024-11-18T05:41:24.743419Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"2. **TypeError**: Happens when the function is used incorrectly, like passing incorrect arguments to methods.","metadata":{}},{"cell_type":"code","source":"soup.find_all(123)  # This will raise a TypeError since `find_all` expects a string\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T05:41:42.334180Z","iopub.execute_input":"2024-11-18T05:41:42.334648Z","iopub.status.idle":"2024-11-18T05:41:42.345879Z","shell.execute_reply.started":"2024-11-18T05:41:42.334607Z","shell.execute_reply":"2024-11-18T05:41:42.344606Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"3. **Parser Errors**: This error can occur if the HTML document is malformed or has invalid tags. Using the `html.parser` or `lxml` parser can help manage these issues better.\r\n\r\n### **8.2 Handling HTTP Errors with `requests` Library**\r\n\r\nSometimes, requests to websites may fail due to various reasons, such as:\r\n\r\n- **404 Not Found**: The requested resource is not available.\r\n- **500 Internal Server Error**: The server encountered an unexpected condition.\r\n- **403 Forbidden**: Access to the resource is denied.\r\n\r\nTo handle these errors, we can use the `requests` library's `raise_for_status()` method, which raises an HTTPError for bad responses.\r\n","metadata":{}},{"cell_type":"code","source":"import requests\nfrom bs4 import BeautifulSoup\n\nurl = \"https://example.com/nonexistent-page\"\ntry:\n    response = requests.get(url)\n    response.raise_for_status()  # Raises HTTPError for 4xx/5xx errors\n    soup = BeautifulSoup(response.text, 'html.parser')\n    print(soup.title.text)\nexcept requests.exceptions.HTTPError as e:\n    print(f\"HTTP Error: {e}\")\nexcept requests.exceptions.ConnectionError as e:\n    print(f\"Connection Error: {e}\")\nexcept requests.exceptions.Timeout as e:\n    print(f\"Timeout Error: {e}\")\nexcept Exception as e:\n    print(f\"An error occurred: {e}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T05:42:56.930223Z","iopub.execute_input":"2024-11-18T05:42:56.931237Z","iopub.status.idle":"2024-11-18T05:42:57.480423Z","shell.execute_reply.started":"2024-11-18T05:42:56.931188Z","shell.execute_reply":"2024-11-18T05:42:57.479382Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **8.3 Using `try-except` Blocks for Error Handling**\r\n\r\nUsing `try-except` blocks is an effective way to catch and handle exceptions in your scraping script.\r\n\r\n**Example: Handling Missing Tags**\r\n","metadata":{}},{"cell_type":"code","source":"html_content = \"<html><body><p>Hello World</p></body></html>\"\nsoup = BeautifulSoup(html_content, 'html.parser')\n\ntry:\n    title = soup.find(\"h1\").text  # This will raise an AttributeError\n    print(title)\nexcept AttributeError:\n    print(\"The 'h1' tag is not found in the HTML content.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T05:43:26.967983Z","iopub.execute_input":"2024-11-18T05:43:26.968828Z","iopub.status.idle":"2024-11-18T05:43:26.975321Z","shell.execute_reply.started":"2024-11-18T05:43:26.968785Z","shell.execute_reply":"2024-11-18T05:43:26.974228Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **8.4 Setting Up Retry Logic for Failed Requests**\r\n\r\nWhen making requests, especially to unreliable servers, your scraper might occasionally fail. Setting up a retry mechanism can help handle transient errors.\r\n\r\n**Using `time.sleep` and `retry` logic:**\r\n","metadata":{}},{"cell_type":"code","source":"import requests\nfrom bs4 import BeautifulSoup\nimport time\n\ndef fetch_data(url, retries=3):\n    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n    attempt = 0\n    while attempt < retries:\n        try:\n            response = requests.get(url, headers=headers, timeout=10)\n            response.raise_for_status()\n            return BeautifulSoup(response.text, 'html.parser')\n        except requests.exceptions.RequestException as e:\n            print(f\"Request failed: {e}. Retrying in 5 seconds...\")\n            attempt += 1\n            time.sleep(5)  # Wait before retrying\n    print(\"Failed to fetch data after multiple attempts.\")\n    return None\n\nurl = \"https://httpbin.org/status/500\"  # Example of an endpoint that may return errors\nsoup = fetch_data(url)\nif soup:\n    print(soup.title)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T05:43:52.928518Z","iopub.execute_input":"2024-11-18T05:43:52.928955Z","iopub.status.idle":"2024-11-18T05:44:10.717267Z","shell.execute_reply.started":"2024-11-18T05:43:52.928916Z","shell.execute_reply":"2024-11-18T05:44:10.716150Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Key Points:**\r\n- We use the `User-Agent` header to avoid getting blocked.\r\n- `time.sleep(5)` introduces a delay between retries.\r\n- `timeout=10` prevents hanging if the server takes too long to respon\n\r\n\r\n### **8.5 Handling Multipage Data with Error Handling**\r\n\r\nIf you are scraping multiple pages, error handling becomes more critical. Let's consider an example where we scrape paginated data with proper error handling.\r\n\r\n","metadata":{}},{"cell_type":"code","source":"import requests\nfrom bs4 import BeautifulSoup\nimport time\n\ndef scrape_multipage(base_url, num_pages):\n    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n    all_data = []\n\n    for page in range(1, num_pages + 1):\n        url = f\"{base_url}?page={page}\"\n        print(f\"Scraping {url}\")\n\n        try:\n            response = requests.get(url, headers=headers, timeout=10)\n            response.raise_for_status()\n            soup = BeautifulSoup(response.text, 'html.parser')\n\n            items = soup.find_all('h2')  # Example: Extracting all 'h2' tags from each page\n            for item in items:\n                all_data.append(item.text.strip())\n\n        except requests.exceptions.RequestException as e:\n            print(f\"Error fetching {url}: {e}\")\n            continue\n\n        time.sleep(2)  # Polite scraping delay\n\n    return all_data\n\nbase_url = \"https://quotes.toscrape.com/page\"\ndata = scrape_multipage(base_url, num_pages=5)\n\nprint(\"Scraped Data:\", data)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-18T05:44:49.226171Z","iopub.execute_input":"2024-11-18T05:44:49.226643Z","iopub.status.idle":"2024-11-18T05:44:52.141661Z","shell.execute_reply.started":"2024-11-18T05:44:49.226597Z","shell.execute_reply":"2024-11-18T05:44:52.140556Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Explanation:**\r\n- We handle errors on each page using `try-except`, so a failure on one page does not stop the entire script.\r\n- We use `time.sleep(2)` between page requests to avoid being blocked.\r\n- **Retry Logic** can be combined here to enhance robustness furthe\n\n## **9. Web Scraping Projects**\r\n\r\n### **9.1 Project 1: Scraping E-commerce Product Data**\r\n\r\n**Objective**: Scrape product names, prices, and ratings from an e-commerce website.\r\n\r\n**URL**: We will use **\"Books to Scrape\"**, a test e-commerce website available at [https://books.toscrape.com/](https://books.toscrape.com/).\r\n\r\n#### **Steps**:\r\n1. Extract product names, prices, and ratings from the website.\r\n2. Handle pagination to scrape data from multiple pages.\r\n3. Save the data to a CSV file.\r\nr.\r\n","metadata":{}},{"cell_type":"code","source":"import requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\n\n# Base URL of the website\nbase_url = \"https://books.toscrape.com/catalogue/page-{}.html\"\n\n# Lists to store the data\nbook_titles = []\nbook_prices = []\nbook_ratings = []\n\n# Loop through multiple pages\nfor page in range(1, 6):  # Scraping first 5 pages\n    print(f\"Scraping page {page}...\")\n    \n    # Get the page content\n    url = base_url.format(page)\n    response = requests.get(url)\n    soup = BeautifulSoup(response.text, 'html.parser')\n    \n    # Find all book containers\n    books = soup.find_all('article', class_='product_pod')\n    \n    # Extract data from each book\n    for book in books:\n        title = book.h3.a['title']\n        price = book.find('p', class_='price_color').text\n        rating = book.p['class'][1]\n        \n        # Append to lists\n        book_titles.append(title)\n        book_prices.append(price)\n        book_ratings.append(rating)\n\n# Save the data to a CSV file\ndata = pd.DataFrame({\n    \"Title\": book_titles,\n    \"Price\": book_prices,\n    \"Rating\": book_ratings\n})\n\ndata.to_csv(\"ecommerce_books.csv\", index=False)\nprint(\"Data saved to ecommerce_books.csv\")\ndata.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T06:03:21.599413Z","iopub.execute_input":"2024-11-19T06:03:21.600029Z","iopub.status.idle":"2024-11-19T06:03:23.919795Z","shell.execute_reply.started":"2024-11-19T06:03:21.599982Z","shell.execute_reply":"2024-11-19T06:03:23.918585Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **Key Points**:\r\n- **Pagination**: We used the format `page-{}.html` to scrape multiple pages.\r\n- **Rating Extraction**: The rating is stored as a CSS class (`star-rating X`), so we extract the second class name.\r\n- **Data Storage**: The scraped data is saved in a CSV file.\r\n\r\n### **9.2 Project 2: Scraping News Headlines and Summaries**\r\n\r\n**Objective**: Scrape news headlines, summaries, and URLs from a news website.\r\n\r\n**URL**: We will use **\"The Hacker News\"**, a popular tech news site, at [https://thehackernews.com/](https://thehackernews.com/).\r\n\r\n#### **Steps**:\r\n1. Extract headlines, summaries, and article URLs.\r\n2. Handle pagination for news articles.\r\n3. Save the data to a CSV file.\r\n\r\n","metadata":{}},{"cell_type":"code","source":"import requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\n\n# URL for scraping\nbase_url = \"https://thehackernews.com/search/label/Cyber%20Attack?max-results=10&start={}\"\n\n# Lists to store the data\nheadlines = []\nsummaries = []\narticle_links = []\n\n# Loop through the first 3 pages\nfor start in range(0, 30, 10):  # 0, 10, 20\n    print(f\"Scraping page starting at {start}...\")\n    \n    url = base_url.format(start)\n    response = requests.get(url)\n    soup = BeautifulSoup(response.text, 'html.parser')\n    \n    # Find articles\n    articles = soup.find_all('div', class_='body-post')\n\n    for article in articles:\n        headline = article.find('h2', class_='home-title').text.strip()\n        summary = article.find('div', class_='home-desc').text.strip()\n        link = article.find('a')['href']\n        \n        # Append to lists\n        headlines.append(headline)\n        summaries.append(summary)\n        article_links.append(link)\n\n# Save the data to a CSV file\ndata = pd.DataFrame({\n    \"Headline\": headlines,\n    \"Summary\": summaries,\n    \"URL\": article_links\n})\n\ndata.to_csv(\"news_data.csv\", index=False)\nprint(\"Data saved to news_data.csv\")\ndata.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T06:03:33.045950Z","iopub.execute_input":"2024-11-19T06:03:33.046348Z","iopub.status.idle":"2024-11-19T06:03:33.170916Z","shell.execute_reply.started":"2024-11-19T06:03:33.046308Z","shell.execute_reply":"2024-11-19T06:03:33.169805Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **Key Points**:\r\n- **Pagination Handling**: The URL pattern uses a `start` parameter to handle pagination.\r\n- **Data Collection**: We collect headlines, summaries, and URLs.\r\n\r\n### **9.3 Project 3: Scraping Weather Data from a Weather Website**\r\n\r\n**Objective**: Extract the current weather information for multiple cities.\r\n\r\n**URL**: We will use **\"Time and Date Weather\"**, available at [https://www.timeanddate.com/weather/](https://www.timeanddate.com/weather/).\r\n\r\n#### **Steps**:\r\n1. Extract city names, temperatures, and weather descriptions.\r\n2. Scrape weather data for multiple cities.\r\n3. Save the data to a CSV file.\r\n\r\n","metadata":{}},{"cell_type":"code","source":"import requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\n\n# Base URL of the website\ncities = [\"karachi\", \"islamabad\", \"multan\", \"quetta\", \"lahore\",\"peshawar\",\"hyderabad\",\"abbottabad\",\"bahawalpur\",\"sialkot\",\"sukkur\",\"murree\",\"gujranwala\"]\nbase_url = \"https://www.timeanddate.com/weather/pakistan/\"\n\n# Lists to store data\ncity_names = []\ntemperatures = []\nhumidities = []\nweatherConditions=[]\npressures = []\ndew_points = []\n\n# Scrape data for each city\nfor city in cities:\n    print(f\"Scraping weather data for {city}...\")\n    \n    # Constructing the URL correctly\n    url = f\"{base_url}{city}\"  # Ensure proper URL formatting\n    response = requests.get(url)\n\n    # Check if the request was successful\n    if response.status_code == 200:\n        soup = BeautifulSoup(response.text, 'html.parser')\n        \n        # Extract data\n        city_name = soup.find('h1').text.split(\"Weather\")[0].strip()\n        temperature = soup.find('div', class_='h2').text.strip()\n        pressure = soup.find('table',class_=\"table table--left table--inner-borders-rows\").find_all(\"tr\")[4].td.text\n        humidity = soup.find('table',class_=\"table table--left table--inner-borders-rows\").find_all(\"tr\")[5].td.text\n        dewPoint = soup.find('table',class_=\"table table--left table--inner-borders-rows\").find_all(\"tr\")[6].td.text\n\n        weatherCondition = soup.find('div',class_=\"bk-focus__qlook\").p.text.split(\".\")[0]\n        \n        \n        # Append to lists\n        city_names.append(city)\n        temperatures.append(temperature)\n        humidities.append(humidity)\n        pressures.append(pressure)\n        dew_points.append(dewPoint)\n        weatherConditions.append(weatherCondition)\n    else:\n        print(f\"Failed to retrieve data for {city}. Status code: {response.status_code}\")\n\n# Save the data to a CSV file if there's any data collected\nif city_names:\n    data = pd.DataFrame({\n        \"City\": city_names,\n        \"Temperature\": temperatures,\n        \"humidity\": humidities,\n        \"Weather Condition\":weatherConditions,\n        \"Pressue\": pressures,\n        \"Dew Point\": dew_points\n        \n    })\n\n    data.to_csv(\"weather_data.csv\", index=False)\n    print(\"Data saved to weather_data.csv\")\n    print(data.head())\nelse:\n    print(\"No data collected.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T06:15:46.856190Z","iopub.execute_input":"2024-11-19T06:15:46.856644Z","iopub.status.idle":"2024-11-19T06:15:48.310100Z","shell.execute_reply.started":"2024-11-19T06:15:46.856604Z","shell.execute_reply":"2024-11-19T06:15:48.308734Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **Key Points**:\r\n- **Dynamic URL Creation**: Using city names to dynamically build the URL for scraping.\r\n- **Extracting Weather Data**: We use specific classes for temperature and description.\r\n\r\n### **9.4 Project 4: Scraping Wikipedia Tables and Articles**\r\n\r\n**Objective**: Scrape data from a Wikipedia table.\r\n\r\n**URL**: We will use the **\"List of Countries by GDP\"** page at [https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)](https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)).\r\n\r\n#### **Steps**:\r\n1. Extract country names, GDP values, and rankings.\r\n2. Parse the HTML table and extract data.\r\n3. Save the data to a CSV file.\r\n\r\n","metadata":{}},{"cell_type":"code","source":"import requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\n\n# URL of the Wikipedia page\nurl = \"https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)\"\nresponse = requests.get(url)\nsoup = BeautifulSoup(response.text, 'html.parser')\n\n# Find the table\ntable = soup.find('table', class_='wikitable')\n\n# Lists to store the data\ncountries = []\ngdps = []\nrankings = []\n\n# Extract data from table rows\nfor row in table.find_all('tr')[1:]:\n    cols = row.find_all('td')\n    if len(cols) > 1:\n        country = cols[1].text.strip()\n        gdp = cols[2].text.strip()\n        rank = cols[0].text.strip()\n        \n        countries.append(country)\n        gdps.append(gdp)\n        rankings.append(rank)\n\n# Save to CSV\ndata = pd.DataFrame({\n    \"Rank\": rankings,\n    \"Country\": countries,\n    \"GDP (Nominal)\": gdps\n})\n\ndata.to_csv(\"gdp_data.csv\", index=False)\nprint(\"Data saved to gdp_data.csv\")\nprint(data.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T06:17:49.237510Z","iopub.execute_input":"2024-11-19T06:17:49.237883Z","iopub.status.idle":"2024-11-19T06:17:49.510654Z","shell.execute_reply.started":"2024-11-19T06:17:49.237850Z","shell.execute_reply":"2024-11-19T06:17:49.509117Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **Key Points**:\r\n- **Table Parsing**: We identify and extract data from specific columns in the table.\r\n- **Data Cleaning**: We strip whitespace and handle text extraction properly.\n## **10. Advanced BeautifulSoup Techniques**\r\n\r#### **10.1 Using Proxies to Avoid IP Blocking**\r\nWhen web scraping, websites may block your IP address if you send too many requests. Using proxies allows you to mask your real IP and rotate between multiple IPs to avoid detection.\r\n\r\n**Steps:**\r\n1. **Choose a Proxy Provider:** Use free proxy lists (e.g., [free-proxy-list.net](https://free-proxy-list.net)) or paid services.\r\n2. **Set Up a Proxy with Requests:**\r\n\r\n\r\n","metadata":{}},{"cell_type":"code","source":"import requests\nfrom bs4 import BeautifulSoup\n\nproxies = {\n    \"http\": \"http://proxy_ip:proxy_port\",\n    \"https\": \"https://proxy_ip:proxy_port\"\n}\nurl = \"http://example.com\"\nresponse = requests.get(url, proxies=proxies)\nsoup = BeautifulSoup(response.content, \"html.parser\")\nprint(soup.prettify())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"3. **Test Proxy Health:** Ensure proxies are functional by testing them with a simple request.\n\n\n### **10.2 Rotating User Agents with `fake_useragent` Library**\nWebsites often block automated scripts by detecting default user agents. Rotating user agents makes your scraper appear more human-like.\n\n**Steps:**\n1. **Install `fake_useragent`:**\n   ```bash\n   pip install fake-useragent\n   ```\n2. **Use Rotating User Agents:**\n","metadata":{}},{"cell_type":"code","source":"from fake_useragent import UserAgent\nimport requests\nfrom bs4 import BeautifulSoup\n\nua = UserAgent()\nheaders = {\"User-Agent\": ua.random}\n\nurl = \"http://example.com\"\nresponse = requests.get(url, headers=headers)\nsoup = BeautifulSoup(response.content, \"html.parser\")\nprint(soup.prettify())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"3. **Integrate with Multiple Requests:**\r\n   Rotate user agents dynamically for each request in your script.\r\n\r\n--\r\n\r\n#### **10.3 Building a Web Scraping Pipeline with Python**\r\nA web scraping pipeline ensures data is scraped, processed, and stored efficiently.\r\n\r\n**Steps:**\r\n1. **Define the Scraping Workflow:**\r\n   - Fetch the webpage.\r\n   - Parse HTML with BeautifulSoup.\r\n   - Extract and clean data.\r\n   - Save data to a file or database.\r\n2. **Example Pipeline:**\r\n","metadata":{}},{"cell_type":"code","source":"import requests\nfrom bs4 import BeautifulSoup\nimport csv\n\ndef fetch_page(url):\n    response = requests.get(url)\n    return response.content\n\ndef parse_html(html):\n    soup = BeautifulSoup(html, \"html.parser\")\n    return soup.find_all(\"div\", class_=\"example-class\")\n\ndef save_to_csv(data, filename=\"output.csv\"):\n    with open(filename, \"w\", newline=\"\") as f:\n        writer = csv.writer(f)\n        writer.writerow([\"Name\", \"Price\"])\n        writer.writerows(data)\n\ndef main():\n    url = \"http://example.com\"\n    html = fetch_page(url)\n    data = parse_html(html)\n    save_to_csv(data)\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T06:25:07.741576Z","iopub.execute_input":"2024-11-19T06:25:07.741987Z","iopub.status.idle":"2024-11-19T06:25:07.916453Z","shell.execute_reply.started":"2024-11-19T06:25:07.741955Z","shell.execute_reply":"2024-11-19T06:25:07.915325Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **10.4 Scraping Data from Websites with Infinite Scroll**\r\nWebsites with infinite scrolling load data dynamically via AJAX. This requires fetching additional pages programmatically.\r\n\r\n**Steps:**\r\n1. **Inspect Network Activity:**\r\n   - Use browser developer tools to find the AJAX URL pattern.\r\n2. **Make Multiple Requests:**\r\n","metadata":{}},{"cell_type":"code","source":"import requests\nfrom bs4 import BeautifulSoup\n\nbase_url = \"https://jsonplaceholder.typicode.com/users?page=\"\nall_data = []\n\nfor page in range(1, 6):  # Adjust page range as needed\n    url = f\"{base_url}{page}\"\n    response = requests.get(url)\n    \n    # Check if the request was successful\n    if response.status_code == 200:\n        data = response.json()  # Assuming JSON response\n        all_data.extend(data)  # Adjust key to your data structure\n    else:\n        print(f\"Failed to retrieve data from {url}\")\n\nprint(all_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T06:27:10.303826Z","iopub.execute_input":"2024-11-19T06:27:10.304239Z","iopub.status.idle":"2024-11-19T06:27:11.843544Z","shell.execute_reply.started":"2024-11-19T06:27:10.304203Z","shell.execute_reply":"2024-11-19T06:27:11.842327Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"3. **Combine AJAX Data with BeautifulSoup:**\n   Process fetched data with BeautifulSoup or directly store it.\n\n\n### **10.5 Using `concurrent.futures` for Parallel Scraping**\nParallel scraping speeds up data collection by making requests concurrently.\n\n**Steps:**\n1. **Import Necessary Modules:**\n2. **Set Up a Function for Scraping:**\n3. **Use a Thread Pool for Parallel Execution:**","metadata":{}},{"cell_type":"code","source":"# 1. **Import Necessary Modules:**\n\nfrom concurrent.futures import ThreadPoolExecutor\nimport requests\nfrom bs4 import BeautifulSoup\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T06:31:51.655016Z","iopub.execute_input":"2024-11-19T06:31:51.655412Z","iopub.status.idle":"2024-11-19T06:31:51.661129Z","shell.execute_reply.started":"2024-11-19T06:31:51.655350Z","shell.execute_reply":"2024-11-19T06:31:51.659753Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 2. **Set Up a Function for Scraping:**\ndef scrape_page(url):\n    response = requests.get(url)\n    soup = BeautifulSoup(response.content, \"html.parser\")\n    return soup.title.string","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T06:31:52.468053Z","iopub.execute_input":"2024-11-19T06:31:52.469260Z","iopub.status.idle":"2024-11-19T06:31:52.474783Z","shell.execute_reply.started":"2024-11-19T06:31:52.469214Z","shell.execute_reply":"2024-11-19T06:31:52.473399Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 3. **Use a Thread Pool for Parallel Execution:**\n\nurls = [\n    \"http://example.com/page1\",\n    \"http://example.com/page2\",\n    \"http://example.com/page3\",\n]\n\nwith ThreadPoolExecutor(max_workers=5) as executor:\n    results = executor.map(scrape_page, urls)\n\nfor title in results:\n    print(title)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T06:31:53.686796Z","iopub.execute_input":"2024-11-19T06:31:53.687197Z","iopub.status.idle":"2024-11-19T06:31:53.866310Z","shell.execute_reply.started":"2024-11-19T06:31:53.687160Z","shell.execute_reply":"2024-11-19T06:31:53.865006Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **11. Web Scraping Best Practices and Optimization**\r\n\r\n### **11.1 Respecting `robots.txt` and Website Terms of Service**\r\n- **What is `robots.txt`?**\r\n  - A `robots.txt` file provides guidelines for web crawlers on what parts of a website they can or cannot access.\r\n- **How to Check `robots.txt`?**\r\n  - Example: Access `https://example.com/robots.txt` in your browser.\r\n- **Respect the Guidelines:**\r\n  - Before scraping, ensure you are allowed to scrape the website sections you target.\r\n- **Ethical Considerations:**\r\n  - Always follow the site's terms of service and avoid scraping sensitive data or overloading serers.\r\n\r\n#### **11.2 Using Request Headers to Avoid Blocking**\r\n- **Why Use Headers?**\r\n  - Many websites block requests without proper headers, such as a User-Agent.\r\n- **Adding Headers with the `requests` Library:**\r\n","metadata":{}},{"cell_type":"code","source":"import requests\n\nheaders = {\n    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n    'Accept-Language': 'en-US,en;q=0.9',\n}\n\n# Replace with a valid URL\nresponse = requests.get('https://jsonplaceholder.typicode.com/posts', headers=headers)\n\n# Check if the request was successful\nif response.status_code == 200:\n    print(response.json())  # Print the JSON response\nelse:\n    print(f\"Failed to retrieve data: {response.status_code}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T06:34:59.596956Z","iopub.execute_input":"2024-11-19T06:34:59.597494Z","iopub.status.idle":"2024-11-19T06:34:59.653524Z","shell.execute_reply.started":"2024-11-19T06:34:59.597443Z","shell.execute_reply":"2024-11-19T06:34:59.652216Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- **Rotating User Agents:**\r\n  - Use the `fake_useragent` library to change User-Agent frequently.\r\n","metadata":{}},{"cell_type":"code","source":"!pip install fake_useragent","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T06:35:57.162720Z","iopub.execute_input":"2024-11-19T06:35:57.163125Z","iopub.status.idle":"2024-11-19T06:36:09.729086Z","shell.execute_reply.started":"2024-11-19T06:35:57.163078Z","shell.execute_reply":"2024-11-19T06:36:09.727731Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from fake_useragent import UserAgent\n\nua = UserAgent()\nheaders = {'User-Agent': ua.random}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T06:36:13.401804Z","iopub.execute_input":"2024-11-19T06:36:13.402227Z","iopub.status.idle":"2024-11-19T06:36:13.430062Z","shell.execute_reply.started":"2024-11-19T06:36:13.402187Z","shell.execute_reply":"2024-11-19T06:36:13.428795Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **11.3 Optimizing the Scraping Process for Speed**\r\n- **Avoiding Unnecessary Requests:**\r\n  - Minimize the number of requests by targeting only essential data.\r\n- **Use Efficient Parsing:**\r\n  - Use lightweight parsers like `lxml` for better performance.\r\n","metadata":{}},{"cell_type":"code","source":"from bs4 import BeautifulSoup\nsoup = BeautifulSoup(response.text, 'lxml')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T06:36:37.405640Z","iopub.execute_input":"2024-11-19T06:36:37.406025Z","iopub.status.idle":"2024-11-19T06:36:37.416225Z","shell.execute_reply.started":"2024-11-19T06:36:37.405981Z","shell.execute_reply":"2024-11-19T06:36:37.415077Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- **Concurrent Scraping:**\r\n  - Use `concurrent.futures` for parallel requests.\r\n","metadata":{}},{"cell_type":"code","source":"import concurrent.futures\nimport requests\n\nheaders = {\n    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n    'Accept-Language': 'en-US,en;q=0.9',\n}\n\nurls = ['https://example1.com', 'https://example2.com']\n\ndef fetch_url(url):\n    response = requests.get(url, headers=headers)\n    return response.content\n\nwith concurrent.futures.ThreadPoolExecutor() as executor:\n    results = executor.map(fetch_url, urls)\n\nfor result in results:\n    print(result)  # Print the content of each fetched URL","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T06:37:27.764546Z","iopub.execute_input":"2024-11-19T06:37:27.764944Z","iopub.status.idle":"2024-11-19T06:37:28.104282Z","shell.execute_reply.started":"2024-11-19T06:37:27.764907Z","shell.execute_reply":"2024-11-19T06:37:28.102623Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **11.4 Data Throttling and Sleep Intervals**\r\n- **Why Throttle Requests?**\r\n  - To avoid overwhelming the server and being flagged as a bot.\r\n- **Using Random Sleep Intervals:**\r\n","metadata":{}},{"cell_type":"code","source":"import time\nimport random\n\nfor url in urls:\n    response = requests.get(url, headers=headers)\n    time.sleep(random.uniform(1, 3))  # Random delay between 1 and 3 seconds\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T06:38:10.425106Z","iopub.execute_input":"2024-11-19T06:38:10.425565Z","iopub.status.idle":"2024-11-19T06:38:15.633539Z","shell.execute_reply.started":"2024-11-19T06:38:10.425528Z","shell.execute_reply":"2024-11-19T06:38:15.632529Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **11.5 Avoiding Captchas with Anti-Captcha Services**\r\n- **Detecting Captchas:**\r\n  - Check if the response contains captcha-related content (e.g., `Recaptcha` tags).\r\n- **Using Anti-Captcha Tools:**\r\n  - Tools like **2Captcha**, **Anti-Captcha**, or **DeathByCaptcha** can solve captchas programmatically.\r\n","metadata":{}},{"cell_type":"code","source":"!pip install anticaptchaofficial","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T06:40:00.499299Z","iopub.execute_input":"2024-11-19T06:40:00.500590Z","iopub.status.idle":"2024-11-19T06:40:11.941705Z","shell.execute_reply.started":"2024-11-19T06:40:00.500542Z","shell.execute_reply":"2024-11-19T06:40:11.940164Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from anticaptchaofficial.recaptchav2proxyless import *\n\nsolver = recaptchaV2Proxyless()\nsolver.set_verbose(1)\nsolver.set_key(\"YOUR_ANTICAPTCHA_API_KEY\")  # Replace with your Anti-Captcha API key\nsolver.set_website_url(\"https://valid-website.com\")  # Replace with the actual website URL\nsolver.set_website_key(\"YOUR_SITE_KEY\")  # Replace with the actual site key for reCAPTCHA\n\ntoken = solver.solve_and_return_solution()\nif token != 0:\n    print(\"Captcha Solved: \" + token)\nelse:\n    print(\"Captcha Error: \" + solver.error_code)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T06:40:19.276631Z","iopub.execute_input":"2024-11-19T06:40:19.277099Z","iopub.status.idle":"2024-11-19T06:40:19.593072Z","shell.execute_reply.started":"2024-11-19T06:40:19.277052Z","shell.execute_reply":"2024-11-19T06:40:19.591860Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **12. Deploying a Web Scraping Script**\r\n\r\nIn this section, we'll explore how to automate, deploy, and make your web scraping script efficient and accessible.--\r\n\r\n#### **12.1 Using Cron Jobs for Scheduling**\r\nCron jobs are used to schedule scripts to run periodically on UNIX-based systems (Linux, macOS).\r\n\r\n1. **Set Up a Cron Job**\r\n   - Open your terminal and edit the crontab:\r\n     ```bash\r\n     crontab -e\r\n     ```\r\n   - Add a line specifying when to run your script. For example, to run it daily at midnight:\r\n     ```bash\r\n     0 0 * * * /usr/bin/python3 /path/to/your_script.py\r\n     ```\r\n2. **Verify Cron Job Execution**\r\n   - Use the `cron.log` file to ensure the script runs correctly.\r\n   - Debug any errors that arise by running the script manually.\r\n\r\n3. **Windows Alternative**\r\n   - Use Task Scheduler for periodiccuton on Windows.\r\n\r\n---\r\n\r\n#### **12.2 Deploying on Cloud Services (AWS Lambda, Heroku)**\r\n\r\n1. **Deploying on AWS Lambda**\r\n   - Package your Python script along with dependencies:\r\n     ```bash\r\n     pip install -r requirements.txt -t .\r\n     zip -r script_package.zip .\r\n     ```\r\n   - Upload the `script_package.zip` to AWS Lambda.\r\n   - Set the handler function in AWS Lambda (e.g., `lambda_function.lambda_handler`).\r\n   - Schedule the script using AWS EventBridge.\r\n\r\n2. **Deploying on Heroku**\r\n   - Create a `Procfile` for Heroku:\r\n     ```plaintext\r\n     web: python your_script.py\r\n     ```\r\n   - Push your code to a Heroku repository:\r\n     ```bash\r\n     heroku create\r\n     git push heroku main\r\n     ``   Use Heroku Scheduler for periodic tasks.\r\n\r\n---\r\n\r\n#### **12.3 Creating a Web Scraping API with FastAPI**\r\n\r\nFastAPI can be used to serve scraped data via an API:\r\n\r\n1. **Install FastAPI and Uvicorn**:\r\n   ```bash\r\n   pip install fastapi uvicorn\r\n   ```\r\n\r\n2. **Create a Simple API**:\r\n","metadata":{}},{"cell_type":"code","source":"from fastapi import FastAPI\nimport requests\nfrom bs4 import BeautifulSoup\n\napp = FastAPI()\n\n@app.get(\"/scrape\")\ndef scrape_data():\n    url = \"https://example.com\"\n    response = requests.get(url)\n    soup = BeautifulSoup(response.content, \"html.parser\")\n    data = [item.text for item in soup.find_all(\"h2\")]\n    return {\"scraped_data\": data}\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"3. **Run the API Locally**:\n   ```bash\n   uvicorn app:app --reload\n   ```\n\n4. **Deploy FastAPI on a Server**\n   - Use services like AWS, Azure, or Heroku for deployment.\n\n\n### **12.4 Sending Notifications (Email/Slack) After Scraping**\n\n1. **Sending Email Notifications**\n   - Use Python's `smtplib`:\n","metadata":{}},{"cell_type":"code","source":"import smtplib\nfrom email.mime.text import MIMEText\n\ndef send_email(subject, message, recipient):\n    sender = \"your_email@example.com\"\n    password = \"your_password\"\n    msg = MIMEText(message)\n    msg[\"Subject\"] = subject\n    msg[\"From\"] = sender\n    msg[\"To\"] = recipient\n\n    with smtplib.SMTP(\"smtp.gmail.com\", 587) as server:\n        server.starttls()\n        server.login(sender, password)\n        server.sendmail(sender, recipient, msg.as_string())\n\nsend_email(\"Scraping Completed\", \"Your scraping task is done.\", \"recipient@example.com\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"2. **Sending Slack Notifications**\r\n   - Use Slack Webhooks:\r\n","metadata":{}},{"cell_type":"code","source":"import requests\n\ndef send_slack_message(webhook_url, message):\n    payload = {\"text\": message}\n    requests.post(webhook_url, json=payload)\n\nwebhook_url = \"https://hooks.slack.com/services/your/webhook/url\"\nsend_slack_message(webhook_url, \"Scraping task completed successfully!\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"3. **Automating Notifications**\r\n   - Integrate the email/Slack function in your scraping script to trigger notifications after data is processed.-\n\r\n\r\n#### **12.5 Using Multiple URLs for Consistent Data**\r\nWhen scraping, if a single URL doesn’t have pagination or becomes unavailable, you can:\r\n   \r\n1. **Use Alternative URLs**:\r\n   - Maintain a list of fallback URLs:\r\n","metadata":{}},{"cell_type":"code","source":"urls = [\n    \"https://example1.com\",\n    \"https://example2.com\",\n    \"https://example3.com\"\n]\nfor url in urls:\n    try:\n        response = requests.get(url)\n        if response.status_code == 200:\n            soup = BeautifulSoup(response.content, \"html.parser\")\n            # Process data\n            break\n    except Exception as e:\n        print(f\"Error with {url}: {e}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T06:45:26.905799Z","iopub.execute_input":"2024-11-19T06:45:26.906199Z","iopub.status.idle":"2024-11-19T06:45:27.615466Z","shell.execute_reply.started":"2024-11-19T06:45:26.906163Z","shell.execute_reply":"2024-11-19T06:45:27.614241Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"2. **Aggregate Data from Multiple URLs**:\r\n   - Combine data from several URLs into a single dataset:\r\n","metadata":{}},{"cell_type":"code","source":"combined_data = []\nfor url in urls:\n    response = requests.get(url)\n    soup = BeautifulSoup(response.content, \"html.parser\")\n    data = [item.text for item in soup.find_all(\"div\", class_=\"data-class\")]\n    combined_data.extend(data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T06:46:27.345643Z","iopub.execute_input":"2024-11-19T06:46:27.346104Z","iopub.status.idle":"2024-11-19T06:46:27.966432Z","shell.execute_reply.started":"2024-11-19T06:46:27.346065Z","shell.execute_reply":"2024-11-19T06:46:27.965242Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"By combining multiple URLs and a robust fallback mechanism, you ensure your scraper remains functional and avoids errors due to unavailable pages.\r\n\n## **13. Web Scraping Alternatives**\r\n\r\n### **13.1 Comparison with Scrapy and Selenium**\r\nBeautifulSoup is excellent for parsing static HTML content, but there are alternatives that may be more efficient depending on your needs:\r\n\r\n1. **Scrapy**  \r\n   - **Advantages**:\r\n     - Built-in support for asynchronous requests, making it faster for large-scale scraping.\r\n     - Framework-like structure with built-in tools for parsing, data storage, and middleware.\r\n     - Automatic handling of crawling through multiple pages.\r\n   - **Disadvantages**:\r\n     - More complex setup than BeautifulSoup.\r\n     - Not ideal for scraping JavaScript-heavy websites.\r\n   \r\n2. **Selenium**  \r\n   - **Advantages**:\r\n     - Can interact with JavaScript-rendered pages and simulate browser behavior.\r\n     - Useful for scraping interactive elements (e.g., dropdown menus, buttons).\r\n   - **Disadvantages**:\r\n     - Slower than BeautifulSoup and Scrapy because it uses a full browser.\r\n     - Requires significant system resources for large-scale scraping.\r\n\r\n3. **BeautifulSoup**  \r\n   - **Advantages**:\r\n     - Simple to use and lightweight.\r\n     - Great for small to medium-sized projects with static content.\r\n   - **Disadvantages**:\r\n     - Lacks asynchronous support.\r\n     - Requires additional libraries (e.g., `reqests`) for fetching data.\r\n\r\n#### **13.2 When to Use BeautifulSoup, Scrapy, or Selenium**\r\n| **Scenario**                               | **Tool**          |\r\n|--------------------------------------------|-------------------|\r\n| Simple static HTML scraping                | BeautifulSoup     |\r\n| Large-scale scraping with many pages       | Scrapy            |\r\n| JavaScript-heavy or dynamic web content    | Selenium          |\r\n| Need both static and dynamic scaping      | Combine Tools     |\r\n\r\n#### **13.3 Exploring Other Libraries**\r\n1. **Puppeteer**  \r\n   - A Node.js library for controlling headless Chrome browsers.\r\n   - Ideal for interacting with dynamic websites.\r\n   - Examples include scraping sites requiring login or handling infinite scrolls.\r\n\r\n2. **Playwright**  \r\n   - Similar to Puppeteer but supports multiple browsers (Chromium, Firefox, WebKit).\r\n   - Allows faster and more reliable scraping for complex websites.\r\n   - Offers built-in support\n\n## **14. Course Project: Building a Complete Web Scraper**\r\n\r\n### **14.1 Project Setup and Requirements**\r\n\r\nFor this project, we will build a complete web scraper using **BeautifulSoup**. The goal is to scrape **product data** (e.g., name, price, rating, availability) from a website. If the chosen website doesn’t support pagination, we will supplement it by scraping additional URLs to ensure our scraper handles diverse case---\r\n\r\n#### **14.2 Building the Scraper with BeautifulSoup**\r\n\r\n1. **Set Up the Environment**  \r\n   - Install required libraries:\r\n     ```bash\r\n     pip install requests beautifulsoup4 pandas\r\n     ```\r\n   - Import libraries in your Python script:\r\n for handling iframes and multi-tab scenarios.\r\n\n\r\n","metadata":{}},{"cell_type":"code","source":"import requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T06:53:37.005991Z","iopub.execute_input":"2024-11-19T06:53:37.006377Z","iopub.status.idle":"2024-11-19T06:53:37.011884Z","shell.execute_reply.started":"2024-11-19T06:53:37.006346Z","shell.execute_reply":"2024-11-19T06:53:37.010488Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"2. **Target Website**  \r\n   Use the **Books to Scrape** website, which is public and legal for scraping:  \r\n   **URL**: [http://books.toscrape.com/](http://books.toscrape.com/)  \r\n   This website contains multiple pages with books, making it ideal for demonstrating both single and multipage scraping.\r\n\r\n3. **Fetch the HTML Content**  \r\n   Start with fetching HTML for the first page:\r\n","metadata":{}},{"cell_type":"code","source":"BASE_URL = \"http://books.toscrape.com/\"\n\nresponse = requests.get(BASE_URL)\nif response.status_code == 200:\n    soup = BeautifulSoup(response.content, 'html.parser')\n    print(\"Page fetched successfully!\")\nelse:\n    print(f\"Failed to fetch page: {response.status_code}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T06:54:43.127604Z","iopub.execute_input":"2024-11-19T06:54:43.128021Z","iopub.status.idle":"2024-11-19T06:54:43.662121Z","shell.execute_reply.started":"2024-11-19T06:54:43.127982Z","shell.execute_reply":"2024-11-19T06:54:43.660633Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"4. **Parse the Data**\r\n   Extract specific data such as book title, price, and stock availability:\r\n","metadata":{}},{"cell_type":"code","source":"# Function to parse a single page\ndef parse_page(soup):\n    books = []\n    for book in soup.find_all('article', class_='product_pod'):\n        title = book.h3.a['title']\n        price = book.find('p', class_='price_color').text\n        availability = book.find('p', class_='instock availability').text.strip()\n        books.append({'Title': title, 'Price': price, 'Availability': availability})\n    return books\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T06:55:02.690631Z","iopub.execute_input":"2024-11-19T06:55:02.691041Z","iopub.status.idle":"2024-11-19T06:55:02.698505Z","shell.execute_reply.started":"2024-11-19T06:55:02.691001Z","shell.execute_reply":"2024-11-19T06:55:02.697193Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"5. **Handle Pagination**\r\n   Create logic to navigate through multiple pages:\r\n","metadata":{}},{"cell_type":"code","source":"def scrape_books(base_url):\n    all_books = []\n    next_page = \"catalogue/page-1.html\"\n    \n    while next_page:\n        print(f\"Scraping {base_url + next_page}\")\n        response = requests.get(base_url + next_page)\n        soup = BeautifulSoup(response.content, 'html.parser')\n        \n        # Parse the current page\n        all_books.extend(parse_page(soup))\n        \n        # Check for 'next' button\n        next_button = soup.find('li', class_='next')\n        next_page = next_button.a['href'] if next_button else None\n    return all_books\n\n# Scrape all pages\nbooks_data = scrape_books(BASE_URL)\nprint(f\"Scraped {len(books_data)} books.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T06:55:21.662737Z","iopub.execute_input":"2024-11-19T06:55:21.663118Z","iopub.status.idle":"2024-11-19T06:55:22.256527Z","shell.execute_reply.started":"2024-11-19T06:55:21.663083Z","shell.execute_reply":"2024-11-19T06:55:22.255276Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **14.3 Data Cleaning and Exporting**\r\n\r\n1. **Cleaning Data**\r\n   Remove unwanted characters and standardize formatting:\r\n","metadata":{}},{"cell_type":"code","source":"for book in books_data:\n    book['Price'] = book['Price'].replace('£', '').strip()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T06:55:50.267186Z","iopub.execute_input":"2024-11-19T06:55:50.267585Z","iopub.status.idle":"2024-11-19T06:55:50.272887Z","shell.execute_reply.started":"2024-11-19T06:55:50.267548Z","shell.execute_reply":"2024-11-19T06:55:50.271479Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"2. **Exporting to CSV**\r\n   Save the cleaned data for further analysis:\r\n","metadata":{}},{"cell_type":"code","source":"df = pd.DataFrame(books_data)\ndf.to_csv('books_data.csv', index=False)\nprint(\"Data exported to books_data.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T06:56:13.846538Z","iopub.execute_input":"2024-11-19T06:56:13.846952Z","iopub.status.idle":"2024-11-19T06:56:13.857577Z","shell.execute_reply.started":"2024-11-19T06:56:13.846914Z","shell.execute_reply":"2024-11-19T06:56:13.856496Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **14.4 Deploying and Automating the Web Scraper**\n\n1. **Using Cron Jobs (Linux/Mac) or Task Scheduler (Windows)**\n   Automate the script to run periodically:\n   - For Linux/Mac, schedule the script using a cron job:\n     ```bash\n     crontab -e\n     # Add a line to run the script every day at 9 AM\n     0 9 * * * /usr/bin/python3 /path/to/your/script.py\n     ```\n   - For Windows, use Task Scheduler to set up periodic execution.\n\n2. **Deploy on a Cloud Service**\n   Use a cloud platform like AWS Lambda or Heroku for deployment:\n   - **Heroku Deployment**:\n     - Create a `requirements.txt` file with all dependencies.\n     - Use `git` to push the project to Heroku.\n\n\n### **14.5 Project Showcase and Review**\n\n- **Summary of Features:**\n  - Scrapes book data (title, price, availability).\n  - Handles multipage scraping dynamically.\n  - Cleans and exports data to CSV.\n  - Automates scraping for regular updates.\n\n- **Key Challenges and Solutions:**\n  - **Challenge**: Missing pages or invalid URLs.  \n    **Solution**: Use try-except blocks to handle errors gracefully.\n  - **Challenge**: Dynamic content (if present).  \n    **Solution**: Integrate Selenium when necessary.\n\n- **Code Overview**:\n  The final script handles real-world challenges like pagination, data cleaning, and exporting while maintaining simplicity.\n\n\n## **15. Conclusion and Next Steps**\n\n### **15.1 Recap of Key Learnings**\nIn this course, we covered everything you need to know about **BeautifulSoup**, starting from basic concepts to advanced techniques. Here’s a summary of what you’ve learned:\n- **Basics of Web Scraping**: Understanding the ethical and legal considerations, setting up your environment, and understanding HTML structure.\n- **Navigating HTML with BeautifulSoup**: Using tags, attributes, and advanced CSS selectors to extract data efficiently.\n- **Handling Complex Scenarios**: Parsing dynamic content using Selenium and handling AJAX requests for JSON data.\n- **Data Cleaning and Exporting**: Cleaning the extracted data and saving it in formats like CSV, JSON, or databases.\n- **Error Handling and Optimization**: Managing HTTP errors, retries, and optimizing scrapers to avoid blocking.\n- **Real-World Projects**: Implementing scraping pipelines for e-commerce, news, and weather data.\n- **Deployment and Automation**: Using cloud services like AWS or scheduling tasks with cron jobs.\n\n### **15.2 Resources for Further Learning**\nTo continue building your skills, consider exploring these resources:\n1. **Books**:\n   - *\"Automate the Boring Stuff with Python\"* by Al Sweigart.\n   - *\"Web Scraping with Python\"* by Ryan Mitchell.\n2. **Online Courses**:\n   - Python-focused courses on platforms like Coursera, Udemy, and edX.\n   - Free tutorials on [Real Python](https://realpython.com/).\n3. **Official Documentation**:\n   - [BeautifulSoup Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n\n### **15.3 Tips for Building Your Own Web Scrapers**\n1. **Start Small**: Test your scraper on a small, static site before scaling to more complex or dynamic sites.\n2. **Understand Robots.txt**: Always check the `robots.txt` file of a website to respect its scraping policies.\n3. **Use Headers and Proxies**: Include appropriate headers like user-agent and rotate proxies to prevent IP bans.\n4. **Handle Pagination Gracefully**: Ensure your scraper can dynamically move through pages, checking for `next` buttons or unique page URLs.\n5. **Test Often**: Websites frequently change their HTML structure, so test and update your scraper regularly.\n\n\n### **15.4 Addressing Pagination Issues with a New URL**\n\nIf your original URL doesn’t contain multiple pages, try using a website that supports pagination. Let’s scrape data from a valid multi-page example.\n\n**Example URL**:  \nWe will use the jobs listing site *Remotive.io*. Here’s the base URL:  \n`https://remotive.io/remote-jobs/software-dev`\n\nThis site contains job listings across multiple pages.\n\n**Updated Steps for Pagination:**\n1. **Identify the Pagination Logic**:\n   - Check the URL structure for pagination. On Remotive, the next pages are loaded dynamically, so you will need to handle AJAX calls or simulate clicks using Selenium.\n\n2. **Code Implementation:**\n\nHere’s a snippet for handling pagination dynamically:\n\n","metadata":{}},{"cell_type":"code","source":"import requests\nfrom bs4 import BeautifulSoup\nimport time\n\nbase_url = \"https://remotive.io/remote-jobs/software-dev\"\n\ndef scrape_jobs(base_url, max_pages=5):\n    current_page = 1\n    all_jobs = []\n\n    while current_page <= max_pages:\n        print(f\"Scraping page {current_page}...\")\n        # Simulate changing page number if applicable\n        response = requests.get(f\"{base_url}?page={current_page}\")\n        \n        if response.status_code != 200:\n            print(f\"Error fetching page {current_page}: {response.status_code}\")\n            break\n\n        soup = BeautifulSoup(response.text, \"html.parser\")\n        \n        # Extract job details\n        jobs = soup.select(\"div.job-tile-title\")\n        for job in jobs:\n            title = job.text.strip()\n            all_jobs.append(title)\n        \n        # Simulate waiting to avoid detection\n        time.sleep(2)\n        \n        current_page += 1\n\n    return all_jobs\n\n# Fetch and print job titles\njob_list = scrape_jobs(base_url)\nfor idx, job in enumerate(job_list, 1):\n    print(f\"{idx}. {job}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T07:01:45.668571Z","iopub.execute_input":"2024-11-19T07:01:45.668958Z","iopub.status.idle":"2024-11-19T07:02:04.187642Z","shell.execute_reply.started":"2024-11-19T07:01:45.668925Z","shell.execute_reply":"2024-11-19T07:02:04.186219Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Challenges Resolved**:\n- **Dynamic Pagination**: The script now dynamically scrapes up to 5 pages, as specified by `max_pages`.\n- **Error Handling**: Errors during HTTP requests are caught gracefully.\n- **Flexible Base URL**: You can replace the `base_url` with other paginated websites.\n\n### **15.5 Q&A and Troubleshooting Common Issues**\n\n**Q1. My scraper suddenly stops working. What should I do?**  \n- Inspect the website's structure to see if it has changed. Use browser developer tools to verify.\n- If the website blocks your IP, try using proxies or VPNs.\n\n**Q2. How can I scrape JavaScript-rendered content?**  \n- Use Selenium or Playwright to interact with the DOM rendered by JavaScript.\n- Alternatively, inspect the network tab for AJAX requests that return raw data.\n\n**Q3. What to do if the site has anti-scraping mechanisms like CAPTCHAs?**  \n- Use services like 2Captcha to bypass CAPTCHAs.\n- Minimize frequent requests by adding random delays.\n\n**Q4. How do I export large datasets?**  \n- Write data incrementally to CSV or database to avoid memory overflow.\n\nWith this comprehensive guide, you now have all the tools and knowledge needed to build robust scrapers for various applications. Best of luck in your web scraping journey!\n","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}